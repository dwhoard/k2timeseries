{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variable stars from the NASA K2 mission\n",
    "\n",
    "[Kepler & K2](https://keplerscience.arc.nasa.gov) provide long-time-baseline, high-precision photometry for exoplanet and astrophysics research.\n",
    "\n",
    "This Notebook contains code written and adapted by D. W. Hoard for processing, cleaning, performing time series analysis, and visualizing K2 time series data for variable stars.\n",
    "\n",
    "\n",
    "## Workflow for processing K2 time series (light curve) data\n",
    "\n",
    "I retrieved the raw \\*\\_llc.fits data files (e.g., ktwo212218649-c16_llc.fits) from the [MAST archive](https://mast.stsci.edu/). I have already pre-processed them into \\*mast-kepconvert.txt data files (e.g., EPIC_212218649_sap_mast-kepconvert.txt) using the separate Python tasks k2sc and kepconvert. For completeness, the basic process for installing those packages and performing the pre-processing is decribed below.\n",
    "\n",
    "\n",
    "## Pre-processing packages\n",
    "\n",
    "This notebook uses the pre-processed data files as input. The pre-procesing is performed using two canned Python routines provided by the Kepler/K2 missions. Eventually, I will incorporate them into this workflow. For now, the pre-processing package installation and steps are described here.\n",
    "\n",
    "\n",
    "### Documentation:\n",
    "\n",
    "- K2SC\n",
    "https://archive.stsci.edu/prepds/k2sc/\n",
    "\n",
    "\n",
    "- KEPCONVERT\n",
    "https://pyke.keplerscience.org/tasks/kepconvert.html\n",
    "\n",
    "\n",
    "### Installation and use of the pre-processing packages\n",
    "\n",
    "k2sc: Install from [GitHub](https://github.com/OxES/k2sc).\n",
    "\n",
    "kepconvert: Installed as part of the [PyKE Python package](https://pyke.keplerscience.org/index.html)\n",
    "\n",
    "If you have installed Anaconda, then k2sc and kepconvert can be installed as follows:\n",
    "~~~~\n",
    "# Install PyKE (for kepconvert)\n",
    "pip install msgpack --upgrade\n",
    "pip install argparse --upgrade\n",
    "pip install pyketools --upgrade\n",
    "\n",
    "# Install K2SC prerequisites:\n",
    "pip install numpy --upgrade\n",
    "pip install scipy --upgrade\n",
    "pip install astropy --upgrade\n",
    "pip install george --upgrade\n",
    "\n",
    "# OPTIONAL (for parallelization):\n",
    "pip install mpia4py --upgrade\n",
    "\n",
    "# Install K2SC\n",
    "cd ~/Applications/  # or installation directory of your choice; ensure that $DIRECTORY/k2sc/bin is in your PATH\n",
    "git clone https://github.com/OxES/k2sc.git\n",
    "cd k2sc\n",
    "python setup.py install --user\n",
    "~~~~\n",
    "\n",
    "### Sample Python command line sequence for pre-processing of input data\n",
    "\n",
    "~~~~\n",
    "### Definitions  \n",
    "# Target K2 (EPIC) ID number  \n",
    "k2id='212218649'  \n",
    "# Campaign number  \n",
    "camid='16'  \n",
    "\n",
    "### Automatically construct additional definitions\n",
    "iname='ktwo'${k2id}'-c'${camid}\n",
    "lcf=${iname}'_llc.fits'\n",
    "\n",
    "### Run k2sc for SAP data\n",
    "fltype='sap'\n",
    "k2sc ${lcf} --campaign ${camid} --flux-type ${fltype} --logfile k2sc_${fltype}.log\n",
    "\n",
    "### Export SAP light curve into ascii file\n",
    "scf0='EPIC_'${k2id}'_mast.fits'\n",
    "scf='EPIC_'${k2id}'_'${fltype}'_mast.fits'\n",
    "mv ${scf0} ${scf}\n",
    "kepconvert ${scf} fits2asc --columns TIME,CADENCE,QUALITY,X,Y,FLUX,ERROR,MFLAGS,TRTIME,TRPOSI --overwrite --verbose --logfile kepconvert_${fltype}.log\n",
    "\n",
    "### Run k2sc for PDC data\n",
    "fltype='pdc'\n",
    "k2sc ${lcf} --campaign ${camid} --flux-type ${fltype} --logfile k2sc_${fltype}.log\n",
    "\n",
    "### Export PDC light curve into ascii file\n",
    "scf0='EPIC_'${k2id}'_mast.fits'\n",
    "scf='EPIC_'${k2id}'_'${fltype}'_mast.fits'\n",
    "mv ${scf0} ${scf}\n",
    "kepconvert ${scf} fits2asc --columns TIME,CADENCE,QUALITY,X,Y,FLUX,ERROR,MFLAGS,TRTIME,TRPOSI --overwrite --verbose --logfile kepconvert_${fltype}.log\n",
    "~~~~\n",
    "\n",
    "\n",
    "## SAP vs. PDC data?\n",
    "\n",
    "In short, using the SAP data is preferred for these purposes because some of the artifact removal applied in the PDC pipeline (which assumes that the stars are intrsinically non-variable, other than brief exoplanet transits) can remove actual variability from my targets-of-interest.\n",
    "\n",
    "See https://keplerscience.arc.nasa.gov/pipeline.html\n",
    "\n",
    "\n",
    "## Important note\n",
    "\n",
    "The k2sc README file is wrong about output column contents. FLUX is a copy of the input (raw) flux, TRTIME is the time-dependent (variability) component of model, and TRPOSI is the position-dependent (systematics) component of model. The calculations performed below are correct."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Import general purpose packages used throughout the workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import time \n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Set global user-defined parameters\n",
    "CUSTOMIZE ANY OF THESE AS NEEDED!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################\n",
    "# TARGET AND DATA IDENTIFIERS\n",
    "# list of unique target identifier string(s)\n",
    "tname = ['212218649', '212213538', '212211859', '212222400']\n",
    "# index of target identifier to use (starts at zero)\n",
    "tidx = 0\n",
    "\n",
    "# K2 campaign id number (used to define default covariance matrix split values for plotting)\n",
    "# (-1 = do not use)\n",
    "#k2camid = -1\n",
    "k2camid = 16\n",
    "\n",
    "# select the input data type (SAP or PDC) - see note above\n",
    "dtype = 'sap'\n",
    "#dtype = 'pdc'\n",
    "\n",
    "# GENERAL CONFIGURATION\n",
    "# path for saving output files\n",
    "SAVEPATH = './output/'\n",
    "\n",
    "# verbosity level of debugging/info comments\n",
    "# (-1 = none, 0 = crucial only, 1 = some, 2 = all)\n",
    "LDEBUG = 1\n",
    "###############################################\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Set global default parameters\n",
    "ALMOST NEVER NEED TO CHANGE THESE!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define part of the output file names base don target identifier and data type\n",
    "name_string = tname[tidx]+'_'+dtype\n",
    "\n",
    "# default dimensions for visualization output files, if not specified in individual \"fig\" setups (inches)\n",
    "PSIZE_DEFAULT = (12,6)\n",
    "plt.rcParams[\"figure.figsize\"] = PSIZE_DEFAULT\n",
    "\n",
    "# optional addition to input path if non-variable \"check\" data is stored in a separate directory\n",
    "import os.path\n",
    "INPUTPATH = ''\n",
    "if tidx > 0 and os.path.exists('nearby_stars'):\n",
    "    INPUTPATH = 'nearby_stars/ktwo'+tname[tidx]+'/'\n",
    "                             \n",
    "# fontsize for (1) tick labels, (2) axis labels\n",
    "size_sm = '14'\n",
    "size_md = '16'\n",
    "size_lg = '18'\n",
    "\n",
    "# K2 start time\n",
    "t0 = 2454833.0\n",
    "\n",
    "# make array of alphabet letters for general use later\n",
    "abclabels = list(string.ascii_lowercase)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Define global functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert file counter to string for creating output file names\n",
    "def strfcount(fcount):\n",
    "    sfcount = str(fcount)\n",
    "    if fcount < 10:\n",
    "        sfcount = '0'+sfcount\n",
    "    return sfcount\n",
    "\n",
    "# normalize series to itself and convert to %\n",
    "def norm_self(x):\n",
    "    return 100*(x/np.median(x) - 1.0)\n",
    "\n",
    "# normalize series to another number and convert to %\n",
    "def norm_other(x,y):\n",
    "    return 100*(x/y - 1.0)\n",
    "\n",
    "# truncate a number to keep only the decimal part (used with phase calculations)\n",
    "def mphi(p):\n",
    "    return p-int(p)\n",
    "\n",
    "# calculate axis limits for plotting edge-to-edge lines\n",
    "# expected input:\n",
    "#   r = 2-element series with [min, max] axis limits\n",
    "#   m = ax.margins()[i] where i = 0 for x axis, 1 for y-axis\n",
    "def axis_limits(r, m):\n",
    "    diff = m*(r[1]-r[0])\n",
    "    return [r[0]-diff, r[1]+diff]\n",
    "\n",
    "# create a timestamp of the current date and time\n",
    "def timestamp():\n",
    "    ts = time.localtime()\n",
    "    return time.strftime(\"%Y-%m-%d %H:%M:%S\", ts)\n",
    "\n",
    "# track runtime\n",
    "# declare start = time.time() before start of process of interest, then invoke time_since(start) at end\n",
    "def time_since(since):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    h = np.floor(s / 3600)\n",
    "    s -= h * 3600\n",
    "    m = np.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dh %dm %ds' % (h, m, s)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Initialize log file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create logfile\n",
    "logfile_name = SAVEPATH+name_string+'_k2timeseries.log'\n",
    "logfile_out = open(logfile_name, 'w')\n",
    "\n",
    "# write initial log file entries\n",
    "print(timestamp(), file=logfile_out)\n",
    "print('### PARAMETERS ###', file=logfile_out)\n",
    "print('[user] target index (tidx) =',tidx, file=logfile_out)\n",
    "print('[user] target ID (tname[tidx]) =',tname[tidx], file=logfile_out)\n",
    "print('[user] K2 campaign ID (k2camid) =',k2camid, file=logfile_out)\n",
    "print('[user] data type (dtype) =',dtype, file=logfile_out)\n",
    "print('[user] output file path (SAVEPATH) =',SAVEPATH, file=logfile_out)\n",
    "print('[user] debug comments level (LDEBUG) =',LDEBUG, file=logfile_out)\n",
    "print('[user] K2 start time (t0) =',t0, file=logfile_out)\n",
    "print('name string (name_string) =',name_string,'\\n', file=logfile_out)\n",
    "logfile_out.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Read unfolded (\"raw\") light curve data into a DataFrame; clean data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# create file handler in read mode \n",
    "file_handler = open(INPUTPATH+\"EPIC_\"+name_string+\"_mast-kepconvert.txt\", \"r\") \n",
    "\n",
    "# define column names and convert data to DataFrame\n",
    "dfcols  = ['time', 'cadence', 'quality', 'xpos', 'ypos', 'rawflux', 'fluxerr', 'mflags', 'trtime', 'trposi']\n",
    "df_lc = ( pd.read_csv(file_handler, sep=' ', usecols=(0,1,2,3,4,5,6,7,8,9), header=None, skiprows=1, names=dfcols, comment='#',\n",
    "        dtype={dfcols[0]:'float64',dfcols[1]:'int64',dfcols[2]:'int64',dfcols[3]:'float64',dfcols[4]:'float64',\n",
    "        dfcols[5]:'float64',dfcols[6]:'float64',dfcols[7]:'int64',dfcols[8]:'float64',dfcols[9]:'float64'}) )\n",
    "\n",
    "# close the file handler \n",
    "file_handler.close() \n",
    "\n",
    "# remove rows with NaNs in important columns\n",
    "df_lc.dropna(subset=['time', 'rawflux', 'fluxerr'], inplace=True)\n",
    "\n",
    "# trtime = time-dependent (variability) component of K2SC model\n",
    "# trposi = position-dependent (systematics) component of K2SC model\n",
    "\n",
    "# this is the raw flux with position-dependent artifacts removed (i.e., time-dependent variability only)\n",
    "df_lc['varflux'] = df_lc['rawflux'] - df_lc['trposi'] + np.mean(df_lc['trposi'])\n",
    "\n",
    "# this is the raw flux with time variability removed (i.e., only position-dependent variability is present)\n",
    "df_lc['posflux'] = df_lc['rawflux'] - df_lc['trtime'] + np.mean(df_lc['trtime'])\n",
    "\n",
    "# this is the residual flux with both components removed\n",
    "df_lc['resflux'] = df_lc['rawflux'] - df_lc['trtime'] + np.mean(df_lc['trtime']) - df_lc['trposi'] + np.mean(df_lc['trposi'])\n",
    "\n",
    "# calulate Kepler magnitude from time variability series (2016AJ....152....5D)\n",
    "df_lc['kp2mag'] = 12.0 + (np.log10(df_lc['varflux']/1.74e5) / (-0.4))\n",
    "\n",
    "# remove rows with poor Quality values or masked values (bad values defined in K2 documentation)\n",
    "df_lc_clean = df_lc[(df_lc.quality < 2.0**19) & (df_lc.mflags == 0)].copy()\n",
    "df_lc_dirty_only = df_lc[(df_lc.quality >= 2.0**19) | (df_lc.mflags > 0)].copy()\n",
    "\n",
    "# write logfile entry\n",
    "logfile_out = open(logfile_name, 'a+')\n",
    "print(timestamp(), file=logfile_out)\n",
    "print('### READ AND CLEAN DATA ###', file=logfile_out)\n",
    "message1 = '*** Finished loading and cleaning input data'\n",
    "print(message1,'\\n', file=logfile_out)\n",
    "logfile_out.close()\n",
    "\n",
    "# end this section gracefully\n",
    "if LDEBUG >=2: \n",
    "    pd.options.display.float_format = '{:f}'.format\n",
    "    print(df_lc_clean)\n",
    "    print(df_lc_dirty_only)\n",
    "if LDEBUG >=0: print(message1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Reference: K2 data flags\n",
    "From the [Kepler Archive Manual](https://archive.stsci.edu/kepler/manuals/archive_manual.pdf) (KDMC-10008-006), written by Susan E. Thompson, Dorothy Fraquelli, Jeffrey E. van Cleve and Douglas A. Caldwell (2 May 2016)\n",
    "\n",
    "|Table 2-3: Bits for the QUALITY and SAP_QUALITY data column|\n",
    "|:---:|\n",
    "\n",
    "Bit    | Value    | <p>Explanation</p>\n",
    ":-----:|---------:|----------------------\n",
    " 1*    |       1  | <p>Attitude Tweak</p>\n",
    " 2*    |       2  | <p>Safe Mode</p>\n",
    " 3*    |       4  | <p>Spacecraft is in coarse point. It is set manually<br>to pad not-in-fine point data.</p>\n",
    " 4*    |       8  | <p>Spacecraft is in Earth point. The first real<br>cadence after Earth point is marked.</p>\n",
    " 5     |      16  | <p>Reaction wheel zero crossing</p>\n",
    " 6*    |      32  | <p>Reaction wheel desaturation event</p>\n",
    " 7*    |      64  | <p>Argabrightening detected across multiple<br>channels on this cadence</p>\n",
    " 8     |     128  | <p>Cosmic Ray was found and corrected in<br>optimal aperture pixel</p>\n",
    " 9*    |     256  | <p>Manual Exclude. The cadence was excluded<br>because of an anomaly.</p>\n",
    "10     |     512  | <p>This bit is unused by Kepler.</p>\n",
    "11     |    1024  | <p>SPSD detected. This bit is flagged on the last<br>non-gapped cadence before the maximum<br>positive change due to the detected SPSD.</p>\n",
    "12     |    2048  | <p>Impulsive outlier removed before cotrending</p>\n",
    "13*    |    4096  | <p>Argabrightening event on specified CCD<br>mod/out detected</p>\n",
    "14     |    8192  | <p>Cosmic Ray detected on collateral pixel<br>row or column in optimal aperture.</p>\n",
    "15*    |   16384  | <p>Detector anomaly flag was raised.</p>\n",
    "16*    |   32768  | <p>Spacecraft is not in fine point.</p>\n",
    "17*    |   65536  | <p>No data collected.</p>\n",
    "18     |  131072  | <p>Rolling Band detected in optimal aperture.</p>\n",
    "19     |  262144  | <p>Rolling Band detected in full mask.</p>\n",
    "20     |  545288  | <p>Possible thruster firing. Not set in Kepler data</p>\n",
    "21     | 1048576  | <p>Thruster firing. Not set in Kepler data.</p>\n",
    "&nbsp; | &nbsp;   | <p>\\* indicates that these cadences are gapped<br>by the pipeline (either in CAL, PA or PDC).<br>The original pixel level data is available in<br>most cases.</p>\n",
    "\n",
    "\n",
    "|K2SC Masking Flags|\n",
    "|:---:|\n",
    "\n",
    "Value       | <p>Explanation</p>\n",
    "------------|------------\n",
    "2\\*\\*0  (1) | <p>one of the K2 quality flags on</p>\n",
    "2\\*\\*1  (2) | <p>flare (reserved but not currently used)</p> \n",
    "2\\*\\*2  (4) | <p>transit (reserved but not currently used)</p>\n",
    "2\\*\\*3  (8) | <p>upwards outlier</p>\n",
    "2\\*\\*4 (16) | <p>downwards outlier</p>\n",
    "2\\*\\*5 (32) | <p>nonfinite flux</p>\n",
    "2\\*\\*6 (64) | <p>a periodic mask applied manually by k2sc (not used)</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Visualize the raw and cleaned input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################\n",
    "# CUSTOMIZE ANY OF THESE AS NEEDED!!!\n",
    "\n",
    "# These are the y-axis range values set for the variable star, for use with nearby comparison stars\n",
    "pyrange_ref = [ [18.550970720633437, 20.18986589095351],\n",
    "                [-54.161889734486195, 119.91825364666558],\n",
    "                [-54.161889734486195, 119.91825364666558],\n",
    "                [-15.289839921753096, 395.244983989217],\n",
    "                [6.8446830749511705, 478.94497451782223],\n",
    "                [-1.8630940586328504, 0.6832127839326857] ]\n",
    "\n",
    "USE_PYRANGE_REF = True\n",
    "\n",
    "# fraction of total range for expanding y-axis endpoints when plotting\n",
    "rangefac = 0.05\n",
    "###############################################\n",
    "\n",
    "fcount = 1 # increment output file counter\n",
    "\n",
    "# define default covariance matrix split values (corresponding to roll angle reversals) for plotting\n",
    "# (these times are relative to t0)\n",
    "if k2camid > 0:\n",
    "    splits = {\n",
    "        3:[2154,2190],\n",
    "        4:[2240,2273],\n",
    "        5:[2344],\n",
    "        6:[2390,2428],\n",
    "        7:[2468.5,2515],\n",
    "        8:[2579,2598],\n",
    "        10:[2772,2800],\n",
    "        12:[2916,2949],\n",
    "        13:[2997,3032],\n",
    "        14:[3086.4,3123.65],\n",
    "        16:[3297,3331]\n",
    "    }\n",
    "    if k2camid not in splits:\n",
    "        print('*** WARNING: splits for campaign '+str(k2camid)+' not defined - ignoring')\n",
    "\n",
    "# set up the multipanel plot\n",
    "npanels=6\n",
    "fig, axes = plt.subplots(npanels, 1, figsize=(10,npanels*3), sharex=True)\n",
    "\n",
    "# write logfile\n",
    "logfile_out = open(logfile_name, 'a+')\n",
    "print(timestamp(), file=logfile_out)\n",
    "print('### INPUT DATA VISUALIZATION ###', file=logfile_out)\n",
    "print('[user] fraction of total range for expanding y-axis endpoints when plotting (rangefac):',rangefac, file=logfile_out)\n",
    "print('[user] using K2 campaign splits? (USE_PYRANGE_REF) =',USE_PYRANGE_REF, file=logfile_out)\n",
    "if tidx > 0 and USE_PYRANGE_REF == True:\n",
    "    print('Reference y-axis ranges for panels 0-'+str(npanels-1)+' (pyrange_ref):', file=logfile_out)\n",
    "    for i, item in enumerate(pyrange_ref): print('Panel '+str(i)+': pyrange = '+str(item), file=logfile_out)\n",
    "print('Plotted y-axis ranges for panels 0-'+str(npanels-1)+' (pyrange ):', file=logfile_out)\n",
    "\n",
    "# centered common axis labels\n",
    "fig.text(0.5, 0.09, 'Time - '+str(t0)+' (d)', ha='center', size=size_lg)\n",
    "\n",
    "# change amount of padding between subplot panels\n",
    "fig.subplots_adjust(wspace=0, hspace=0.1)\n",
    "\n",
    "# function for calculating y-axis plotting range\n",
    "def pyrange_calc(y1, y2, rangefac):\n",
    "    pyrange_temp = [min(y1), max(y2)]\n",
    "    pyrange_temp = [pyrange_temp[0]-rangefac*abs(pyrange_temp[0]-pyrange_temp[1]), pyrange_temp[1]+rangefac*abs(pyrange_temp[0]-pyrange_temp[1])]\n",
    "    return pyrange_temp\n",
    "            \n",
    "# loop over panels\n",
    "pyrange=[0,0]\n",
    "ylpad = 10   # padding for y-axis labels (in points)\n",
    "for i, ax in enumerate(axes.flatten()):\n",
    "    if LDEBUG >= 2: print(i, ax)\n",
    "    \n",
    "    ax.tick_params(labelsize=size_sm)\n",
    "\n",
    "    # Position\n",
    "    if i == npanels-1:\n",
    "        pyrange_temp = pyrange_calc([min(df_lc.xpos),min(df_lc.ypos)], [max(df_lc.xpos),max(df_lc.xpos)], rangefac)\n",
    "        if tidx > 0 and USE_PYRANGE_REF == True:\n",
    "            pyrange = [(j - np.mean(pyrange_ref[i]) + np.mean(pyrange_temp)) for j in pyrange_ref[i]]\n",
    "        else:\n",
    "            pyrange = pyrange_temp\n",
    "        ax.set_ylim(pyrange)\n",
    "        ax.plot(df_lc_clean.time-t0,df_lc_clean.xpos, 'k.', markersize=2)\n",
    "        ax.plot(df_lc_clean.time-t0,df_lc_clean.ypos, 'r.', markersize=2)\n",
    "        ax.set_ylabel('$\\Delta$pixel', size=size_lg, labelpad=ylpad)\n",
    "        ax.annotate(' (a) X and Y (red) Position', size=size_md, \n",
    "            xy=(ax.get_xlim()[1], 0.5*(pyrange[1]-pyrange[0])+pyrange[0]), xycoords='data',\n",
    "            horizontalalignment='left', verticalalignment='center')\n",
    "    \n",
    "    # Raw Flux\n",
    "    if i == npanels-2:\n",
    "        #pyrange_temp = [min(df_lc.rawflux), max(df_lc.rawflux)]\n",
    "        pyrange_temp = pyrange_calc(df_lc.rawflux, df_lc.rawflux, rangefac)\n",
    "        if tidx > 0 and USE_PYRANGE_REF == True:\n",
    "            pyrange = [(j - np.mean(pyrange_ref[i]) + np.mean(pyrange_temp)) for j in pyrange_ref[i]]\n",
    "        else:\n",
    "            pyrange = pyrange_temp\n",
    "        ax.set_ylim(pyrange)\n",
    "        ax.plot(df_lc_clean.time-t0,df_lc_clean.rawflux, 'k.', markersize=2)\n",
    "        ax.plot(df_lc_dirty_only.time-t0,df_lc_dirty_only.rawflux, 'r.', markersize=2)\n",
    "        ax.set_ylabel('e$^{-}$/s', size=size_lg, labelpad=ylpad)\n",
    "        ax.annotate(' (b) Raw Flux (red = rejected)', size=size_md, \n",
    "            xy=(ax.get_xlim()[1], 0.5*(pyrange[1]-pyrange[0])+pyrange[0]), xycoords='data',\n",
    "            horizontalalignment='left', verticalalignment='center')\n",
    "\n",
    "    # Variability Components\n",
    "    if i == npanels-3:\n",
    "        #pyrange_temp = [min([min(df_lc_clean.trtime),min(df_lc_clean.trposi)]), max([max(df_lc_clean.trtime),max(df_lc_clean.trposi)])]\n",
    "        pyrange_temp = pyrange_calc([min(df_lc_clean.trtime),min(df_lc_clean.trposi)], [max(df_lc_clean.trtime),max(df_lc_clean.trposi)], rangefac)\n",
    "        if tidx > 0 and USE_PYRANGE_REF == True:\n",
    "            pyrange = [(j - np.mean(pyrange_ref[i]) + np.mean(pyrange_temp)) for j in pyrange_ref[i]]\n",
    "        else:\n",
    "            pyrange = pyrange_temp\n",
    "        ax.set_ylim(pyrange)\n",
    "        ax.plot(df_lc_clean.time-t0,df_lc_clean.trtime, 'k.', markersize=2)\n",
    "        ax.plot(df_lc_clean.time-t0,df_lc_clean.trposi, 'r.', markersize=2)\n",
    "        ax.set_ylabel('e$^{-}$/s', size=size_lg, labelpad=ylpad)\n",
    "        ax.annotate(' (c) Time and Position (red) Components', size=size_md, \n",
    "            xy=(ax.get_xlim()[1], 0.5*(pyrange[1]-pyrange[0])+pyrange[0]), xycoords='data',\n",
    "            horizontalalignment='left', verticalalignment='center')\n",
    "\n",
    "    # Residual Flux\n",
    "    if i == npanels-4:\n",
    "        #pyrange_temp = [min(norm_self(df_lc_clean.varflux)), max(norm_self(df_lc_clean.varflux))]\n",
    "        pyrange_temp = pyrange_calc(norm_self(df_lc_clean.varflux), norm_self(df_lc_clean.varflux), rangefac)\n",
    "        if tidx > 0 and USE_PYRANGE_REF == True:\n",
    "            pyrange = [(j - np.mean(pyrange_ref[i]) + np.mean(pyrange_temp)) for j in pyrange_ref[i]]\n",
    "        else:\n",
    "            pyrange = pyrange_temp\n",
    "        ax.set_ylim(pyrange)\n",
    "        ax.plot(df_lc_clean.time-t0, norm_self(df_lc_clean.resflux), 'k.', markersize=2)\n",
    "        ax.set_ylabel('%', size=size_lg, labelpad=ylpad)\n",
    "        ax.annotate(' (d) Residual Flux', size=size_md, \n",
    "            xy=(ax.get_xlim()[1], 0.5*(pyrange[1]-pyrange[0])+pyrange[0]), xycoords='data',\n",
    "            horizontalalignment='left', verticalalignment='center')\n",
    "\n",
    "    # Time Variable Flux\n",
    "    if i == npanels-5:\n",
    "        #pyrange_temp = [min(norm_self(df_lc_clean.varflux)), max(norm_self(df_lc_clean.varflux))]\n",
    "        pyrange_temp = pyrange_calc(norm_self(df_lc_clean.varflux), norm_self(df_lc_clean.varflux), rangefac)\n",
    "        if tidx > 0 and USE_PYRANGE_REF == True:\n",
    "            pyrange = [(j - np.mean(pyrange_ref[i]) + np.mean(pyrange_temp)) for j in pyrange_ref[i]]\n",
    "        else:\n",
    "            pyrange = pyrange_temp\n",
    "        ax.set_ylim(pyrange)\n",
    "        ax.plot(df_lc_clean.time-t0, norm_self(df_lc_clean.varflux), 'k.', markersize=2)\n",
    "        ax.set_ylabel('%', size=size_lg, labelpad=ylpad)\n",
    "        ax.annotate(' (e) Time Variable Flux', size=size_md, \n",
    "            xy=(ax.get_xlim()[1], 0.5*(pyrange[1]-pyrange[0])+pyrange[0]), xycoords='data',\n",
    "            horizontalalignment='left', verticalalignment='center')\n",
    "\n",
    "    # Time Variable Flux as Kp2 magnitudes\n",
    "    if i == npanels-6:\n",
    "        #pyrange_temp = [max(df_lc_clean.kp2mag), min(df_lc_clean.kp2mag)]\n",
    "        pyrange_temp = pyrange_calc(df_lc_clean.kp2mag, df_lc_clean.kp2mag, rangefac)\n",
    "        if tidx > 0 and USE_PYRANGE_REF == True:\n",
    "            pyrange = [(j - np.mean(pyrange_ref[i]) + np.mean(pyrange_temp)) for j in pyrange_ref[i]]\n",
    "        else:\n",
    "            pyrange = pyrange_temp\n",
    "        ax.set_ylim(pyrange)\n",
    "        ax.plot(df_lc_clean.time-t0, df_lc_clean.kp2mag, 'k.', markersize=2)\n",
    "        ax.invert_yaxis()\n",
    "        ax.set_ylabel('mag', size=size_lg, labelpad=ylpad)\n",
    "        ax.annotate(' (f) Kp2 Magnitude', size=size_md, \n",
    "            xy=(ax.get_xlim()[1], 0.5*(pyrange[1]-pyrange[0])+pyrange[0]), xycoords='data',\n",
    "            horizontalalignment='left', verticalalignment='center')\n",
    "        \n",
    "    # plot decorations (apply to all subplot panels)\n",
    "    # mark covariance matrix splits with a line that spans subplot box limits without padding\n",
    "    if k2camid in splits:\n",
    "        old_ax_margins = ax.margins()\n",
    "        for j in splits[k2camid]:\n",
    "            ax.margins(y=0)\n",
    "            ax.plot([j,j], axis_limits(pyrange,old_ax_margins[1]), 'k:', linewidth=1)\n",
    "    \n",
    "    message1 = 'Panel '+str(i)+': pyrange = '+str(pyrange)\n",
    "    print(message1)\n",
    "    print(message1, file=logfile_out)\n",
    "    \n",
    "    # turn on minor tick marks\n",
    "    ax.minorticks_on()\n",
    "    \n",
    "    # properties of tick marks\n",
    "    ax.tick_params(which='major', direction='out', length=8, bottom=True, top=False, left=True, right=False)\n",
    "    ax.tick_params(which='minor', direction='out', length=4, bottom=True, top=False, left=True, right=False)\n",
    "    \n",
    "# output the visualization\n",
    "plt.savefig(SAVEPATH+name_string+'_'+strfcount(fcount)+'_input_data.png',dpi=150, bbox_inches='tight')\n",
    "\n",
    "# finish logfile entry\n",
    "message2 = '*** Finished visualizing the raw and processed input data'\n",
    "print(message2,'\\n', file=logfile_out)\n",
    "logfile_out.close()\n",
    "\n",
    "# end this section gracefully\n",
    "if LDEBUG >= 0: print('\\n',message2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Set up for Lomb-Scargle periodogram calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################\n",
    "# CUSTOMIZE ANY OF THESE AS NEEDED!!!\n",
    "\n",
    "# override automatic calculation of maximum frequency\n",
    "# (use -1 to ignore this setting)\n",
    "user_fmax = -1\n",
    "###############################################\n",
    "\n",
    "# slice out the periodogram input data\n",
    "x = df_lc_clean.time.to_numpy(copy=True)\n",
    "N = len(x)\n",
    "\n",
    "# store these values for using later\n",
    "varflux_mean = np.mean(df_lc_clean.varflux) # mean of the cleaned varflux\n",
    "varflux_std  = np.std(df_lc_clean.varflux)  # standard deviation of the cleaned varflux\n",
    "\n",
    "if LDEBUG >= 2: print('varflux_mean, varflux_std: ', varflux_mean, varflux_std)\n",
    "\n",
    "# normalize amplitude data to mean = 0, variance = 1\n",
    "y = (df_lc_clean.varflux.to_numpy(copy=True) - varflux_mean) / varflux_std\n",
    "\n",
    "if LDEBUG >= 1: print('Mean, Variance (expect 0, 1):', abs(round(np.mean(y),3)), round(np.var(y),3))\n",
    "\n",
    "# define frequency parameters for periodogram calculation\n",
    "delta_x = [x[i+1] - x[i] for i in range(N-2)]   # list of time differences between points\n",
    "\n",
    "if LDEBUG >= 2: print(min(delta_x), max(delta_x), np.mean(delta_x))\n",
    "\n",
    "# Calculate minimum valid frequency for periodogram (fmin)\n",
    "# fmax corresponds to 1 cycle over the entire duration of the data set\n",
    "# fmin = 1/T, where T = length of data set (time difference between first and last points)\n",
    "T = x[-1]-x[0]\n",
    "fmin = (1.0/T)\n",
    "\n",
    "# Calculate maximum valid frequency for periodogram (fmax)\n",
    "# fmax is based on the Nyquist criterion using the smallest separation of data points as an estimate\n",
    "# (or, to make calculating the periodogram more manageable, set fmax to a smaller frequency that \n",
    "# makes sense for the expected periods)\n",
    "if user_fmax < 0:\n",
    "    fmax = (1.0/(2.0*min(delta_x)))\n",
    "else: \n",
    "    if LDEBUG >= 0: print('User override: maximum frequncy set to', user_fmax)\n",
    "    fmax = user_fmax\n",
    "    \n",
    "# Number of frequencies for which to calculate the periodogram (see References)\n",
    "Nf = int(5.0 * T * fmax)\n",
    "\n",
    "if LDEBUG >= 2: print('T, fmin, fmax, 1/fmin, 1/fmax, Nf: ', T, fmin, fmax, 1/fmin, 1/fmax, Nf)\n",
    "if LDEBUG >= 1:\n",
    "    print('Length of data set:', T)\n",
    "    print('Min/Max frequency & period:', fmin, fmax, 1/fmin, 1/fmax)\n",
    "    print('Number of frequencies in periodogram:', Nf)\n",
    "\n",
    "# scale factor to convert frequencies to angular frequencies\n",
    "scfac = 2.0*np.pi\n",
    "\n",
    "# create an array of linearly spaced frequencies\n",
    "f = [(fmin + i*(fmax-fmin)/Nf) for i in range(Nf)]\n",
    "w = [(fmin + i*(fmax-fmin)/Nf)*scfac for i in range(Nf)]\n",
    "\n",
    "# write logfile\n",
    "logfile_out = open(logfile_name, 'a+')\n",
    "print(timestamp(), file=logfile_out)\n",
    "print('### PERIODOGRAM ###', file=logfile_out)\n",
    "print('[user] override automatic calculation of maximum frequency? (user_fmax) =',user_fmax, file=logfile_out)\n",
    "print('number of points in time series (N) =',N, file=logfile_out)\n",
    "print('mean of the cleaned time series (varflux_mean) =',varflux_mean, file=logfile_out)\n",
    "print('standard deviation of the cleaned time series (varflux_std) =',varflux_std, file=logfile_out)\n",
    "print('mean, variance of normalized time series (expect 0, 1):', abs(round(np.mean(y),3)), round(np.var(y),3), file=logfile_out)\n",
    "print('min, max, mean of successive time differences:', min(delta_x), max(delta_x), np.mean(delta_x), file=logfile_out)\n",
    "print('total length of time series (T):',T, file=logfile_out)\n",
    "print('minimum valid frequency, period for periodogram (fmin):',fmin,1.0/fmin, file=logfile_out)\n",
    "print('maximum valid frequency, period for periodogram (fmax):',fmax,1.0/fmax, file=logfile_out)\n",
    "print('number of frequencies in periodogram (Nf):',Nf, file=logfile_out)\n",
    "message1 = '*** Finished setting up for Lomb-Scargle periodogram'\n",
    "print(message1,'\\n', file=logfile_out)\n",
    "logfile_out.close()\n",
    "\n",
    "# end this section gracefully\n",
    "if LDEBUG >= 0: print('\\n', message1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Calculate Lomb-Scargle periodogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fcount = 2 # increment output file counter\n",
    "\n",
    "# import specific packages needed here\n",
    "from scipy import signal\n",
    "from matplotlib.ticker import MultipleLocator\n",
    "\n",
    "# calculate periodogram\n",
    "# normalization = multiply periodogram amplitude by (2 / len(x))\n",
    "start = time.time()\n",
    "npgram = signal.lombscargle(x, y, w, normalize=True)\n",
    "\n",
    "# visualize periodogram\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(f, npgram)\n",
    "\n",
    "# plot decorations\n",
    "y_minor_tick = 0.005\n",
    "ax.axis([0,np.ceil(max(f)),-2*y_minor_tick,max(npgram)+2*y_minor_tick])\n",
    "ax.minorticks_on()\n",
    "ax.yaxis.set_minor_locator(MultipleLocator(y_minor_tick))\n",
    "plt.xticks(fontsize=size_sm)\n",
    "plt.yticks(fontsize=size_sm)\n",
    "plt.xlabel('Frequency (d$^{-1}$)',fontsize=size_lg, labelpad=10)\n",
    "plt.ylabel('Normalized Amplitude', fontsize=size_lg, labelpad=20)\n",
    "\n",
    "# output visualization\n",
    "plt.savefig(SAVEPATH+name_string+'_'+strfcount(fcount)+'_LSperiodogram.png',dpi=150, bbox_inches='tight')\n",
    "stop = time_since(start)\n",
    "\n",
    "# write logfile\n",
    "logfile_out = open(logfile_name, 'a+')\n",
    "print(timestamp(), file=logfile_out)\n",
    "message1 = '*** Finished calculating L-S periodogram '+str(stop)\n",
    "print(message1,'\\n', file=logfile_out)\n",
    "logfile_out.close()\n",
    "\n",
    "# end this section gracefully\n",
    "if LDEBUG >= 0: print(message1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Bootstrap the false alarm probability (FAP)\n",
    "\n",
    "The FAP provides a means of evaluating the likelihood that a given peak in the periodogram is real (i.e., the inverse of the probability that the peak could have arisen by chance). The FAP is determined by performing a Monte Carlo simulation with many trials. In each trial, a new periodogram is calculated, with the input amplitude data randomly scrambled to different time values in each trial. The cumulative distribution of maximum peak heights in the trials results yields the FAP.\n",
    "\n",
    "Because of the large number of trials, this step can be time consuming. The code has been written to allow for parallel processing on multiple cores. This step can also be skipped entirely, in which case a file of previouly generated peak heights is read in the next step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "###############################################\n",
    "# CUSTOMIZE ANY OF THESE AS NEEDED!!!\n",
    "\n",
    "# select whether (True) or not (False) to run the Monte Carlo simulation, which is time-consuming\n",
    "do_bootstrap = False\n",
    "#do_bootstrap = True\n",
    "\n",
    "# define parameters for FAP Monte Carlo simulation\n",
    "\n",
    "# NOTE: even if do_bootstrap=False, the value of Np calculated below from the\n",
    "# value of fap_desired is used to select the FAP data file to read in the next cell.\n",
    "\n",
    "# 0.01 = 1% false positive rate\n",
    "fap_desired = 0.0001  # = 0.01% Np = 100,000\n",
    "#fap_desired = 0.0002  # = 0.02% Np =  50,000\n",
    "#fap_desired = 0.0005  # = 0.05% Np =  20,000\n",
    "#fap_desired = 0.001   # = 0.1%  Np =  10,000\n",
    "\n",
    "# number of processor cores to use (should be <= actual number of available cores!)\n",
    "# current maximum = 6 but can be extended by adding more \"proc\" statements below\n",
    "nproc = 6\n",
    "###############################################\n",
    "\n",
    "fcount = 3 # increment output file counter\n",
    "\n",
    "# calculate number of Monte Carlo trials based on desired precision of FAP probabilities\n",
    "Np = int(10/fap_desired)\n",
    "\n",
    "# write logfile\n",
    "logfile_out = open(logfile_name, 'a+')\n",
    "print(timestamp(), file=logfile_out)\n",
    "print('### BOOTSTRAPPING THE FAP ###', file=logfile_out)\n",
    "print('[user] run the Monte Carlo simulation? (do_bootstrap):',do_bootstrap, file=logfile_out)\n",
    "print('[user] desired false positive rate (fap_desired):',fap_desired, file=logfile_out)\n",
    "print('[user] number of processor cores used (nproc):',nproc, file=logfile_out)\n",
    "print('number of Monte Carlo trials to reach fap_desired (Np):',Np, file=logfile_out)\n",
    "\n",
    "if do_bootstrap == True:\n",
    "    # import specific packages\n",
    "    from random import SystemRandom\n",
    "    from random import shuffle\n",
    "    import copy\n",
    "    from multiprocessing import Process, Queue\n",
    "\n",
    "    # function for running on multiple processor cores\n",
    "    def boostrap_func(x, y, w):\n",
    "        # create shuffled version of amplitude data (y)\n",
    "        y_trial = copy.copy(y)\n",
    "        r.shuffle(y_trial)\n",
    "        # calcualte trial periodogram using shuffled data\n",
    "        npgram_trial = signal.lombscargle(x, y_trial, w, normalize=True)\n",
    "        # save maximum peak height of trial periodogram as output of trial\n",
    "        q.put(max(npgram_trial))\n",
    "\n",
    "    # number of trials to perform with each processor core\n",
    "    Np_multi = round(Np/nproc)\n",
    "    \n",
    "    if LDEBUG >= 2: print(fap_desired, Np, Np_multi, Np_multi*nproc)\n",
    "\n",
    "    # implement OS-level randomization (better than python internal)\n",
    "    r = SystemRandom()\n",
    "\n",
    "    # run Monte Carlo simulation\n",
    "    max_npgram_trial = []\n",
    "    start = time.time()\n",
    "    for j in range(Np_multi):\n",
    "        # create a queue to share results\n",
    "        q = Queue()\n",
    "        # create nproc sub-processes to do the work\n",
    "        if (nproc >= 1):\n",
    "            proc1 = Process(target=boostrap_func, args=(x, y, w))\n",
    "            proc1.start()\n",
    "        if (nproc >= 2):\n",
    "            proc2 = Process(target=boostrap_func, args=(x, y, w))\n",
    "            proc2.start()\n",
    "        if (nproc >= 3):\n",
    "            proc3 = Process(target=boostrap_func, args=(x, y, w))\n",
    "            proc3.start()\n",
    "        if (nproc >= 4):\n",
    "            proc4 = Process(target=boostrap_func, args=(x, y, w))\n",
    "            proc4.start()\n",
    "        if (nproc >= 5):\n",
    "            proc5 = Process(target=boostrap_func, args=(x, y, w))\n",
    "            proc5.start()\n",
    "        if (nproc >= 6):\n",
    "            proc6 = Process(target=boostrap_func, args=(x, y, w))\n",
    "            proc6.start()\n",
    "        \n",
    "        results = []\n",
    "        # grab values from the queue, one for each process\n",
    "        for i in range(nproc):\n",
    "            # set block=True to block until we get a result\n",
    "            results.append(q.get(True))\n",
    "    \n",
    "        # join results from invidiual processors\n",
    "        if (nproc >= 1):\n",
    "            proc1.join()\n",
    "        if (nproc >= 2):\n",
    "            proc2.join()\n",
    "        if (nproc >= 3):\n",
    "            proc3.join()\n",
    "        if (nproc >= 4):\n",
    "            proc4.join()\n",
    "        if (nproc >= 5):\n",
    "            proc5.join()\n",
    "        if (nproc >= 6):\n",
    "            proc6.join()\n",
    "    \n",
    "        if LDEBUG >= 2: print(j, results)\n",
    "\n",
    "        # create list of trial periodogram maximum peak heights for FAP statistics\n",
    "        for k in range(nproc):\n",
    "            max_npgram_trial.append(results[k])\n",
    "        \n",
    "        if LDEBUG >= 1: \n",
    "            if (j+1) <= 20 or (j+1) % 100 == 0:\n",
    "                print(time_since(start), j+1, (j+1)*nproc, len(max_npgram_trial), max_npgram_trial[j], min(max_npgram_trial), max(max_npgram_trial))\n",
    "\n",
    "    if LDEBUG >= 1: print(time_since(start), j+1, (j+1)*nproc, len(max_npgram_trial), max_npgram_trial[j], min(max_npgram_trial), max(max_npgram_trial))\n",
    "    stop1 = time_since(start)\n",
    "    message1 = '*** Finished bootstrapping iterations  '+str(stop1)\n",
    "    if LDEBUG >= 0: print(message1)\n",
    "\n",
    "    # write bootstrap results to output file\n",
    "    fname1 = SAVEPATH+name_string+'_'+strfcount(fcount)+'_bootstrap_max_npgram_trials_'+str(Np)+'.txt'\n",
    "    outfile = open(fname1, 'w')\n",
    "    for row in max_npgram_trial:\n",
    "        print(str(row), file=outfile)\n",
    "    outfile.close()\n",
    "    stop2 = time_since(start)\n",
    "    message2 = '*** Finished writing file of bootstrap results  '+str(stop2)\n",
    "\n",
    "    # write logfile\n",
    "    print(message1, file=logfile_out)\n",
    "    print(message2, file=logfile_out)\n",
    "        \n",
    "    # end this section gracefully\n",
    "    if LDEBUG >= 0: print(message2)\n",
    "else:\n",
    "    message3 = '*** Skipping bootstrap simulation - read file of previous bootstrap results instead'\n",
    "    # write logfile\n",
    "    print(message3,'\\n', file=logfile_out)\n",
    "    \n",
    "    if LDEBUG >= 0: print(message3)\n",
    "        \n",
    "logfile_out.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Determine FAP confidence levels from bootstrap results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "###############################################\n",
    "# CUSTOMIZE ANY OF THESE AS NEEDED!!!\n",
    "\n",
    "# define histogram bins for peak heights\n",
    "minbin = 0.002   # should be less than the actual minimum peak height in FAP M.C. simulation\n",
    "maxbin = 0.015   # should be greater than the actual maximum peak height in FAP M.C. simulation\n",
    "nhbins = 1000    # a reasonably large number, but avoid having bins with <10 counts\n",
    "\n",
    "# desired FAP confidence levels to evaluate\n",
    "fap_levels = ( [\n",
    "0.9973,   # 3-sigma confidence\n",
    "0.9545,   # 2-sigma confidence\n",
    "0.90,     # 90% confidence\n",
    "0.6827,   # 1-sigma confidence\n",
    "0.50      # 50% confidence\n",
    "])\n",
    "###############################################\n",
    "\n",
    "fcount = 4 # increment output file counter\n",
    "\n",
    "# read FAP Monte Carlo results file\n",
    "fname1 = SAVEPATH+name_string+'_'+strfcount(fcount-1)+'_bootstrap_max_npgram_trials_'+str(Np)+'.txt'\n",
    "max_npgram_trial = np.loadtxt(fname1, usecols=(0))\n",
    "\n",
    "if LDEBUG >= 1: print('Min/Max peak height in bootstrap simulation:', min(max_npgram_trial), max(max_npgram_trial))\n",
    "\n",
    "# define histogram parameters\n",
    "hdstep = (maxbin-minbin)/nhbins   # histogram bin width\n",
    "hbins = [(minbin + hdstep*xx) for xx in range(nhbins+1)]   # histogram bin centers\n",
    "\n",
    "if LDEBUG >= 1: print('Min/Max histogram bins for peak heights:', hbins[0], hbins[-1])\n",
    "\n",
    "# evaluate the histogram\n",
    "values, base = np.histogram(max_npgram_trial, bins=hbins)\n",
    "# evaluate the cumulative probability function\n",
    "cumulative = np.cumsum(values)\n",
    "# plot the cumulative probability function\n",
    "plt.plot(base[:-1], cumulative/max(cumulative), c='blue', zorder=3)\n",
    "# plot the survival probability function\n",
    "#plt.plot(base[:-1], 1.0-cumulative/max(cumulative), c='red')\n",
    "\n",
    "# find peak heights at various confidence levels\n",
    "fap_i = []\n",
    "fap_x = []\n",
    "for lval in fap_levels:\n",
    "    idx = np.where(cumulative/max(cumulative) >= lval)\n",
    "    fap_i.append(idx[0][0])\n",
    "    fap_x.append(base[fap_i[-1]])\n",
    "\n",
    "if LDEBUG >= 2: print(idx90, idx95, idx99)\n",
    "if LDEBUG >= 1: \n",
    "    print('Np = ', Np)\n",
    "    print('Peak height limits for FAP confidence levels of:')\n",
    "    print([str(f\"{ii*100:.2f}\")+'% = '+str(f\"{jj:.6f}\") for ii, jj in zip(fap_levels, fap_x)])\n",
    "\n",
    "# visualize the cumulative probability distribution\n",
    "plt.axis([0.002, 0.009, -0.04, 1.04])\n",
    "\n",
    "# plot decorations\n",
    "plt.minorticks_on()\n",
    "plt.xticks(fontsize=size_sm)\n",
    "plt.yticks(fontsize=size_sm)\n",
    "plt.xlabel('Normalized Periodogram Maximum Peak Height',fontsize=size_lg, labelpad=10)\n",
    "plt.ylabel('Normalized Cumulative Counts', fontsize=size_lg, labelpad=20)\n",
    "# y = 0, 1 lines (zorder controls order in which lines are drawn; lower = earlier)\n",
    "plt.axhline(0, 0, 1, c='black', ls='--', lw=1, zorder=1)\n",
    "\n",
    "# confidence level markers\n",
    "for i, fl in enumerate(fap_levels):\n",
    "    plt.plot([minbin, fap_x[i]], [fl, fl], c='black', ls='-', lw=1)\n",
    "    plt.plot([fap_x[i], fap_x[i]], [-1, fl], c='black', ls='-', lw=1)\n",
    "\n",
    "# output the visualization\n",
    "plt.savefig(SAVEPATH+name_string+'_'+strfcount(fcount)+'_fap_cumulative_trials_'+str(Np)+'.png',dpi=150, bbox_inches='tight')\n",
    "\n",
    "# write logfile\n",
    "logfile_out = open(logfile_name, 'a+')\n",
    "print(timestamp(), file=logfile_out)\n",
    "print('### FAP CONFIDENCE LEVELS ###', file=logfile_out)\n",
    "print('FAP bootstrap simulation results file:',fname1, file=logfile_out)\n",
    "print('minimum, maximum peak height in bootstrap simulation:',min(max_npgram_trial),max(max_npgram_trial), file=logfile_out)\n",
    "print('[user] minimum, maximum peak height bins in histogram (minbin, maxbin) =',minbin,maxbin, file=logfile_out)\n",
    "print('number of points in FAP simulation (Np):',Np, file=logfile_out)\n",
    "print('[user] number of histogram bins (nhbins) =',nhbins, file=logfile_out)\n",
    "print('[user] desired FAP confidence levels to evaluate (fap_levels) =',fap_levels, file=logfile_out)\n",
    "print('    Fiducial FAP confidence levels:', file=logfile_out)\n",
    "print('        0.9973 = 3-sigma', file=logfile_out)\n",
    "print('        0.9545 = 2-sigma', file=logfile_out)\n",
    "print('        0.90   = 90%', file=logfile_out)\n",
    "print('        0.6827 = 1-sigma', file=logfile_out)\n",
    "print('        0.50   = 50%', file=logfile_out)\n",
    "print('Peak height limits for FAP confidence levels of:', file=logfile_out)\n",
    "print([str(f\"{ii*100:.2f}\")+'% = '+str(f\"{jj:.6f}\") for ii, jj in zip(fap_levels, fap_x)], file=logfile_out)\n",
    "message1 = '*** Finished determining FAP levels from bootstrap simulation results'\n",
    "print(message1,'\\n', file=logfile_out)\n",
    "logfile_out.close()\n",
    "\n",
    "# end this section gracefully\n",
    "if LDEBUG >= 0: print('\\n', message1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Identify and characterize the peaks in the periodogram\n",
    "Use Gaussian fitting to find centers (frequencies) and hwhm (uncertainties) of the periodogram peaks above a threshhold value. Assign a FAP confidence level based on the height of each peak."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################\n",
    "# CUSTOMIZE ANY OF THESE AS NEEDED!!!\n",
    "\n",
    "# set height threshold for finding peaks\n",
    "# [-1 = find all peaks down to lowest defined FAP confidence level]\n",
    "phlim = fap_x[-1]\n",
    "\n",
    "# change plot dimensions\n",
    "plt.rcParams[\"figure.figsize\"] = (20,10)\n",
    "\n",
    "# show individual Gaussian fitting peak plots\n",
    "show_plots = False\n",
    "#show_plots = True\n",
    "###############################################\n",
    "\n",
    "# import specific packages needed here\n",
    "from scipy.optimize import curve_fit\n",
    "from scipy.signal import find_peaks\n",
    "\n",
    "# define Gaussian fitting function\n",
    "def gaussfunc(gx,ga,gx0,gsigma):\n",
    "    return ga*np.exp(-(gx-gx0)**2/(2*gsigma**2))\n",
    "\n",
    "# visualize the unfolded light curve data (diagnostic only - not intended for publication)\n",
    "plt.subplots_adjust(hspace=0.35)\n",
    "plt.subplot(3, 1, 1)\n",
    "plt.plot(x-x[0], y, 'b+')\n",
    "plt.xlabel('Time (d)', size=size_sm)\n",
    "plt.ylabel('Amplitude', size=size_sm)\n",
    "plt.minorticks_on()\n",
    "\n",
    "# visualize the periodogram (diagnostic only - not intended for publication)\n",
    "plt.subplots_adjust(hspace=0.35)\n",
    "plt.subplot(3, 1, 2)\n",
    "plt.plot(f, npgram)\n",
    "plt.xlabel('Frequency (d$^{-1}$)', size=size_sm)\n",
    "plt.ylabel('Normalized Amplitude', size=size_sm)\n",
    "plt.axis([fmin, fmax, 0, 1.1*max(npgram)])\n",
    "\n",
    "# find peaks in the periodogram using height threshhold from FAP plot (above)\n",
    "peaks, _ = find_peaks(npgram, height=phlim)\n",
    "\n",
    "if LDEBUG >= 2: \n",
    "    for i, ipeak in enumerate(peaks):\n",
    "        print(ipeak, '  ', f[ipeak], '  ', 1.0/f[ipeak], '  ', npgram[ipeak])\n",
    "\n",
    "# define DataFrame for holding periodogram peaks info\n",
    "peaks_colnames = ['height', 'frequency', 'sigma', 'HWHM', 'period', 'unc', 'FAPconf', 'j', 'kmin', 'kmax', 'npts']\n",
    "df_pd_peaks = pd.DataFrame(columns=peaks_colnames)\n",
    "\n",
    "# Start the peak-fitting process\n",
    "# Loop through the identified peaks\n",
    "peak_fit_center = []\n",
    "for i in range(len(peaks)):\n",
    "    j = peaks[i]\n",
    "    \n",
    "    # assign FAP score\n",
    "    for ii in range(len(fap_x)):\n",
    "        if LDEBUG >= 2: print(ii, j, npgram[j], fap_x[ii], fap_levels[ii])\n",
    "        if npgram[j] >= fap_x[ii]:\n",
    "            fap_out = '>'+str(f\"{fap_levels[ii]*100:.2f}\")+'%'\n",
    "            break\n",
    "\n",
    "    # Find range around peak for the Gaussian fit (start at peak and count points \n",
    "    # in both directions until the \"next\" point is higher than the current one)\n",
    "    # find range shortward of peak\n",
    "    kmin = j\n",
    "    while npgram[kmin-1] < npgram[kmin]:\n",
    "        kmin = kmin-1\n",
    "    # find range longward of peak\n",
    "    kmax = j\n",
    "    while npgram[kmax+1] < npgram[kmax]:\n",
    "        kmax = kmax+1\n",
    "    kmax += 1\n",
    "    \n",
    "    # define sub-arrays for Gaussian fitting\n",
    "    gx = f[kmin:kmax]\n",
    "    gy = npgram[kmin:kmax]\n",
    "    gn = len(gx)                          \n",
    "    gmean = sum(gx*gy)/sum(gy)               \n",
    "    gsigma = np.sqrt(sum(gy * (gx - gmean)**2) / sum(gy))\n",
    "    \n",
    "    # perform Gaussian fitting and recover fit parameters\n",
    "    popt,pcov = curve_fit(gaussfunc,gx,gy,p0=[npgram[j],gmean,gsigma])\n",
    "    if LDEBUG >= 2: \n",
    "        print([npgram[j],gmean,gsigma])\n",
    "        print(popt)\n",
    "    rampl = round(popt[0],8)\n",
    "    rcent = round(popt[1],8)\n",
    "    rsigm = round(popt[2],8)\n",
    "    rhwhm = round(rsigm*np.sqrt(2.0*np.log(2.0)),8)\n",
    "    \n",
    "    rperd = round(1.0/popt[1],8)\n",
    "    rpunc = round(((rsigm*np.sqrt(2.0*np.log(2.0)))/popt[1] * (1.0/popt[1])), 8)\n",
    "    \n",
    "    peak_fit_center.append(rcent)\n",
    "    \n",
    "    # optionally show zoomed plots of individual peaks with overlaid Gaussian fitting results\n",
    "    if show_plots == True:\n",
    "        plt.subplot(3, 1, 3)\n",
    "        plt.plot(f, npgram)\n",
    "        plt.xlabel('Frequency (d$^{-1}$)', size=size_sm)\n",
    "        plt.ylabel('Normalized Amplitude', size=size_sm)\n",
    "        plt.axis([f[kmin-10], f[kmax+10], 0, 1.1*max(npgram[kmin:kmax])])\n",
    "        if LDEBUG >= 2: \n",
    "            print(f[kmin-10], f[kmax+10], 0, 1.1*max(npgram[kmin:kmax]))\n",
    "            for ii in peaks:\n",
    "                plt.plot(f[ii], npgram[ii], \"rx\")\n",
    "        plt.plot(gx,gaussfunc(gx,*popt),'ro:',label='fit')\n",
    "        plt.pause(0.01)\n",
    "   \n",
    "    # append fit parameters to DataFrame\n",
    "    df_params = [rampl, rcent, rsigm, rhwhm, rperd, rpunc, fap_out, int(j), int(kmin), int(kmax), int(kmax-kmin)]\n",
    "    df_pd_peaks = df_pd_peaks.append(pd.Series(df_params, index=df_pd_peaks.columns), ignore_index=True)\n",
    "\n",
    "# print table of peak fit parameters\n",
    "pd.options.display.float_format = '{:.8f}'.format\n",
    "print(df_pd_peaks.iloc[:, [0,1,3,4,5,6]])\n",
    "\n",
    "# restore default plot dimensions\n",
    "plt.rcParams[\"figure.figsize\"] = (12,6)\n",
    "\n",
    "# restore default plot size\n",
    "plt.rcParams[\"figure.figsize\"] = PSIZE_DEFAULT\n",
    "\n",
    "# write logfile\n",
    "logfile_out = open(logfile_name, 'a+')\n",
    "print(timestamp(), file=logfile_out)\n",
    "print('### PERIODOGRAM PEAKS ###', file=logfile_out)\n",
    "print('[user] height threshold for finding peaks (phlim):',phlim, file=logfile_out)\n",
    "print(df_pd_peaks.iloc[:, [0,1,3,4,5,6]], file=logfile_out)\n",
    "message1 = '*** Finished identification and gaussian fits of periodogram peaks'\n",
    "print(message1,'\\n', file=logfile_out)\n",
    "logfile_out.close()\n",
    "\n",
    "# end this section gracefully\n",
    "if LDEBUG >= 2: print(df_pd_peaks)\n",
    "if LDEBUG >= 0: print('\\n', message1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Find frequency relationships"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################\n",
    "# CUSTOMIZE ANY OF THESE AS NEEDED!!!\n",
    "\n",
    "# Define spin frequency peak index\n",
    "i = 11\n",
    "\n",
    "# Define orbital frequency peak index\n",
    "j = 8\n",
    "\n",
    "# sigma-difference limit for identifying a potential frequency relationship\n",
    "siglim = 1.0\n",
    "###############################################\n",
    "\n",
    "# Output selected spin & orbital periods\n",
    "s = df_pd_peaks.frequency[i]\n",
    "es = df_pd_peaks.HWHM[i]\n",
    "spacer = ''\n",
    "if i < 10: spacer = ' '\n",
    "print('   Spin frequency['+str(i)+'] '+spacer+'[s] =', s, '+/-', es)\n",
    "\n",
    "o = df_pd_peaks.frequency[j]\n",
    "eo = df_pd_peaks.HWHM[j]\n",
    "spacer = ''\n",
    "if j < 10: spacer = ' '\n",
    "print('Orbital frequency['+str(j)+'] '+spacer+'[o] =', o, '+/-', eo)\n",
    "\n",
    "# find frequency with smallest difference from given value\n",
    "def find_min_diff(frq):\n",
    "    diff = abs(frq - df_pd_peaks.frequency)\n",
    "    return np.argmin(np.array(diff))\n",
    "\n",
    "# print results\n",
    "def freq_compare(lbl,frq,err,frq_obs,err_obs,logfile_out):\n",
    "    diff = frq_obs - frq   # difference\n",
    "    ediff = np.sqrt(err_obs**2 + err**2)   # difference uncertainty\n",
    "    sigma_diff = diff/ediff   # difference as multiple of uncertainty (\"sigma difference\")\n",
    "    spacer2 = ' '\n",
    "    if imin < 10: spacer2 += ' '\n",
    "    messageX0 = '\\n%12s =  %.8f +/- %.8f' %(lbl, frq, err)\n",
    "    print(messageX0)\n",
    "    print(messageX0, file=logfile_out)\n",
    "    if abs(sigma_diff) <= siglim:\n",
    "        pdiff = 100.0*diff/frq   # percentage difference (relative to frq)\n",
    "        epdiff = abs((100.0*diff/frq)*np.sqrt((ediff/diff)**2 + (err/frq)**2))   # percentage error of percentage difference\n",
    "        spacer = ' '\n",
    "        if diff < 0: spacer = ''\n",
    "        messageX1 = spacer2+'frq_obs[%i] =  %.8f +/- %.8f' %(imin, frq_obs, err_obs)\n",
    "        messageX2 = '        diff =%s %.8f +/- %.8f (%.2f sigma; %.2f%% +/- %.2f%%)' %(spacer, diff, ediff, sigma_diff, pdiff, epdiff) \n",
    "        print(messageX1)\n",
    "        print(messageX2)\n",
    "        print(messageX1, file=logfile_out)\n",
    "        print(messageX2, file=logfile_out)\n",
    "    else:\n",
    "        messageX3 = '***WARNING: No potential frequency relationship found within |sigma_diff| <= '+str(siglim)\n",
    "        print(messageX3)\n",
    "        print(messageX3, file=logfile_out)\n",
    "    return sigma_diff\n",
    "\n",
    "# define possible frequency relationships\n",
    "lbl = []\n",
    "frq = []\n",
    "err = []\n",
    "\n",
    "lbl.append('2o')\n",
    "frq.append(2*o)\n",
    "err.append(2*eo)\n",
    "\n",
    "lbl.append('2s')\n",
    "frq.append(2*s)\n",
    "err.append(2*es)\n",
    "\n",
    "lbl.append('s-o')\n",
    "frq.append(s-o)\n",
    "err.append(np.sqrt(eo**2 + es**2))\n",
    "\n",
    "lbl.append('2(s-o)')\n",
    "frq.append(2*(s-o))\n",
    "err.append(np.sqrt(eo**2 + es**2) * 2)\n",
    "\n",
    "lbl.append('3(s-o)')\n",
    "frq.append(3*(s-o))\n",
    "err.append(np.sqrt(eo**2 + es**2) * 3)\n",
    "\n",
    "lbl.append('4(s-o)')\n",
    "frq.append(4*(s-o))\n",
    "err.append(np.sqrt(eo**2 + es**2) * 4)\n",
    "\n",
    "lbl.append('2s - o')\n",
    "frq.append(2*s-o)\n",
    "err.append(np.sqrt((2.0*es)**2 + eo**2))\n",
    "\n",
    "lbl.append('3s - 2o')\n",
    "frq.append(3*s-2*o)\n",
    "err.append(np.sqrt((3.0*es)**2 + (2.0*eo)**2))\n",
    "\n",
    "lbl.append('4s - 3o')\n",
    "frq.append(4*s-3*o)\n",
    "err.append(np.sqrt((4.0*es)**2 + (3.0*eo)**2))\n",
    "\n",
    "lbl.append('s+o')\n",
    "frq.append(s+o)\n",
    "err.append(np.sqrt(eo**2 + es**2))\n",
    "\n",
    "lbl.append('2s + o')\n",
    "frq.append(2*s+o)\n",
    "err.append(np.sqrt((2.0*es)**2 + eo**2))\n",
    "\n",
    "lbl.append('3s + o')\n",
    "frq.append(3*s+o)\n",
    "err.append(np.sqrt((3.0*es)**2 + eo**2))\n",
    "\n",
    "lbl.append('2o - s')\n",
    "frq.append(2*o-s)\n",
    "err.append(np.sqrt((2.0*eo)**2 + es**2))\n",
    "\n",
    "lbl.append('3o - 2s')\n",
    "frq.append(3*o-2*s)\n",
    "err.append(np.sqrt((3.0*eo)**2 + (2.0*es)**2))\n",
    "\n",
    "lbl.append('4o - 3s')\n",
    "frq.append(4*o-3*s)\n",
    "err.append(np.sqrt((4.0*eo)**2 + (3.0*es)**2))\n",
    "\n",
    "lbl.append('o - (s-o)/8')\n",
    "frq.append(o-(s-o)/8)\n",
    "err.append(np.sqrt(eo**2 + (es/8.0)**2 + (eo/8.0)**2))\n",
    "\n",
    "lbl.append('o + (s-o)/8')\n",
    "frq.append(o+(s-o)/8)\n",
    "err.append(np.sqrt(eo**2 + (es/8.0)**2 + (eo/8.0)**2))\n",
    "\n",
    "lbl.append('s - (s-o)/5')\n",
    "frq.append(s-(s-o)/5)\n",
    "err.append(np.sqrt(es**2 + (es/5.0)**2 + (eo/5.0)**2))\n",
    "\n",
    "lbl.append('s - (s-o)/8')\n",
    "frq.append(s-(s-o)/8)\n",
    "err.append(np.sqrt(es**2 + (es/8.0)**2 + (eo/8.0)**2))\n",
    "\n",
    "lbl.append('s + (s-o)/8')\n",
    "frq.append(s+(s-o)/8)\n",
    "err.append(np.sqrt(es**2 + (es/8.0)**2 + (eo/8.0)**2))\n",
    "\n",
    "lbl.append('o/16')\n",
    "frq.append(o/16)\n",
    "err.append(eo/16)\n",
    "\n",
    "# write logfile\n",
    "logfile_out = open(logfile_name, 'a+')\n",
    "print(timestamp(), file=logfile_out)\n",
    "print('### FREQUENCY RELATIONSHIPS ###', file=logfile_out)\n",
    "print('[user] spin frequency index [i]:', i, file=logfile_out)\n",
    "print('[user] orbital frequency index [j]:', j, file=logfile_out)\n",
    "print('[user] sigma-difference limit for identifying a potential frequency relationship [siglim]:',siglim, file=logfile_out)\n",
    "print('spin frequency['+str(i)+'] '+spacer+'[s] =', s, '+/-', es, file=logfile_out)\n",
    "print('orbital frequency['+str(j)+'] '+spacer+'[o] =', o, '+/-', eo, file=logfile_out)\n",
    "\n",
    "# do the math!\n",
    "imin_used = [i,j]   # keep track of used frequncies\n",
    "for k in range(len(lbl)):\n",
    "    imin = find_min_diff(frq[k])\n",
    "    sigma_diff = freq_compare(lbl[k],frq[k],err[k],df_pd_peaks.frequency[imin],df_pd_peaks.HWHM[imin],logfile_out)\n",
    "    if LDEBUG >= 2: print(k, sigma_diff)\n",
    "    if abs(sigma_diff) <= siglim:\n",
    "        if imin in imin_used:\n",
    "            message1 = '***WARNING: Frequency already used, frq_obs['+str(imin)+'] = '+str(df_pd_peaks.frequency[imin])\n",
    "            print(message1)\n",
    "            print(message1, file=logfile_out)\n",
    "        else: imin_used.append(imin)\n",
    "    \n",
    "# note any unused frequencies\n",
    "imin_possible = range(len(df_pd_peaks.frequency))\n",
    "start, end = imin_possible[0], imin_possible[-1]\n",
    "imin_unused = sorted(set(range(start, end + 1)).difference(imin_used))\n",
    "if LDEBUG >= 2: print('\\n', start, end, imin_possible, imin_used, imin_unused)\n",
    "if len(imin_unused) > 0:\n",
    "    message2 = '\\nUnused frequencies:'\n",
    "    print(message2)\n",
    "    print(message2, file=logfile_out)\n",
    "    for k in imin_unused:\n",
    "        spacer3 = ''\n",
    "        if k < 10: spacer3 = ' '\n",
    "        message3 = ' frq_obs['+str(k)+'] '+spacer3+'= '+str(df_pd_peaks.frequency[k])+'  (period = '+str(round(1.0/df_pd_peaks.frequency[k],8))+')'\n",
    "        print(message3)\n",
    "        print(message3, file=logfile_out)\n",
    "else: \n",
    "    message4 = '\\nNo unused frequencies!'\n",
    "    print(message4)\n",
    "    print(message4, file=logfile_out)\n",
    "\n",
    "# finish logfile entry\n",
    "message5 = '*** Finished identifying potential frequency relationships (for |sigma_diff| <= '+str(siglim)+')'\n",
    "print(message5,'\\n', file=logfile_out)\n",
    "logfile_out.close()\n",
    "\n",
    "# end this section gracefully\n",
    "if LDEBUG >= 0: print('\\n', message5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Visualize periodogram with expanded scale and identified peaks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################\n",
    "# CUSTOMIZE ANY OF THESE AS NEEDED!!!\n",
    "\n",
    "# mark peaks down to specified element of fap_levels with longer (blue) marker\n",
    "# (all other peaks will be marked with shorter (red) marker)\n",
    "faplim = 2  ### this is the 90% confidence level\n",
    "\n",
    "# number of panels to show\n",
    "npanels = 3\n",
    "\n",
    "# update this dictionary to include settings for each panel\n",
    "#    xmin, xmax = minimum, maximum frequency to plot\n",
    "#    ymin, ymax = minimum, maximum power to plot\n",
    "#    y_minor_tick = size of y-axis minor tick intervals\n",
    "if tidx == 0:\n",
    "    panparms = ( {\n",
    "    'xmin':[0.0,  5.1, 10.4],\n",
    "    'xmax':[0.8,  5.9, 11.2],\n",
    "    'ymin':[0.0,  0.0,  0.0],\n",
    "    'ymax':[0.20, 0.20, 0.20],\n",
    "    'y_minor_tick':[0.010, 0.010, 0.010]\n",
    "    } )\n",
    "if tidx == 1:\n",
    "    panparms = ( {\n",
    "    'xmin':[0.0,  5.1, 10.4],\n",
    "    'xmax':[0.8,  5.9, 11.2],\n",
    "    'ymin':[0.0,  0.0,  0.0],\n",
    "    'ymax':[0.005, 0.005, 0.005],\n",
    "    'y_minor_tick':[0.001, 0.001, 0.001]\n",
    "    } )\n",
    "###############################################\n",
    "\n",
    "fcount = 5 # increment output file counter\n",
    "\n",
    "# handle obvious error(s)\n",
    "if tidx > len(tname)-1:\n",
    "    if LDEBUG >= 0: print('ERROR: No plot window settings defined')\n",
    "\n",
    "# set up the multipanel plot\n",
    "fig, axes = plt.subplots(npanels, 1, figsize=(10,npanels*3))\n",
    "\n",
    "# centered common axis labels\n",
    "fig.text(0.5, 0.04, 'Frequency (d$^{-1}$)', ha='center', size=size_lg)\n",
    "fig.text(0.02, 0.5, 'Normalized Amplitude', va='center', rotation='vertical', size=size_lg)\n",
    "\n",
    "# set font size for axis tick labels\n",
    "plt.rc('xtick', labelsize=size_sm) \n",
    "plt.rc('ytick', labelsize=size_sm) \n",
    "\n",
    "# loop over panels\n",
    "for i, ax in enumerate(axes.flatten()):\n",
    "    # plot data into current panel\n",
    "    if LDEBUG >= 2: print(i, ax)\n",
    "    ax.plot(f, npgram, color='black', linewidth=1)\n",
    "\n",
    "    # plot decorations\n",
    "    pstep = panparms['y_minor_tick'][i]\n",
    "    ax.axis([panparms['xmin'][i], panparms['xmax'][i], panparms['ymin'][i]-2*panparms['y_minor_tick'][i], panparms['ymax'][i]+2*panparms['y_minor_tick'][i]], fontsize=size_sm)\n",
    "    ax.minorticks_on()\n",
    "    ax.yaxis.set_minor_locator(MultipleLocator(panparms['y_minor_tick'][i]))\n",
    "    \n",
    "    # plot markers for the gaussian fit peak centers from the previous cell\n",
    "    for ii in range(len(peak_fit_center)):\n",
    "        j = peaks[ii]\n",
    "        if LDEBUG >= 2: print(j, peaks[ii], npgram[j])\n",
    "        if npgram[j] >= fap_x[faplim]:\n",
    "            ax.plot([peak_fit_center[ii],peak_fit_center[ii]], [npgram[j]-pstep,npgram[j]+2*pstep], color='blue')\n",
    "        else:\n",
    "            ax.plot([peak_fit_center[ii],peak_fit_center[ii]], [npgram[j]-pstep,npgram[j]+pstep], color='red')\n",
    "\n",
    "# output the visualization\n",
    "plt.savefig(SAVEPATH+name_string+'_'+strfcount(fcount)+'_LSperiodogram_panels.png',dpi=150, bbox_inches='tight')\n",
    "\n",
    "# write logfile\n",
    "logfile_out = open(logfile_name, 'a+')\n",
    "print(timestamp(), file=logfile_out)\n",
    "print('### PERIODOGRAM PEAKS VISUALIZATION ###', file=logfile_out)\n",
    "message1 = '*** Finished plotting periodogram peaks (marker split at FAP level', '>'+str(f\"{fap_levels[faplim]*100:.2f}\")+'%)'\n",
    "print(message1,'\\n', file=logfile_out)\n",
    "logfile_out.close()\n",
    "\n",
    "if LDEBUG >=2:\n",
    "    print(fap_x)\n",
    "    print(fap_levels)\n",
    "\n",
    "# end this section gracefully\n",
    "if LDEBUG >= 0: print(message1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Calculate the phases for all periods\n",
    "Converts data time values into phases corresponding to all of the periodogram peaks identified above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "###############################################\n",
    "# CUSTOMIZE ANY OF THESE AS NEEDED!!!\n",
    "\n",
    "# OPTIONAL phase offset to add to calculated phases\n",
    "phi_offset = 0.0\n",
    "\n",
    "# OPTIONAL time zeropoint for calculating phases (-1 to use first time)\n",
    "phi_zeropoint = -1\n",
    "###############################################\n",
    "\n",
    "if phi_zeropoint < 0: phi_zeropoint = df_lc_clean.time.to_numpy(copy=True)[0]\n",
    "\n",
    "if LDEBUG >= 2: \n",
    "    print('phi_zeropoint =', phi_zeropoint)\n",
    "    print(df_lc_clean.time.to_numpy(copy=True))\n",
    "\n",
    "# loop over periods\n",
    "for i in range(len(df_pd_peaks.period)):\n",
    "    colname = str(i)\n",
    "    #if i < 100: colname = '0'+colname\n",
    "    if i < 10: colname = '0'+colname\n",
    "    colname = 'phi'+colname\n",
    "    \n",
    "    if LDEBUG >= 2: \n",
    "        print(colname)\n",
    "        print(df_pd_peaks.period.to_numpy(copy=True)[i])\n",
    "    \n",
    "    # add new column for each set of phases to existing DataFrame\n",
    "    df_lc_clean[colname] = (df_lc_clean.time.to_numpy(copy=True) - phi_zeropoint)/df_pd_peaks.period.to_numpy(copy=True)[i] + phi_offset\n",
    "\n",
    "# write logfile\n",
    "logfile_out = open(logfile_name, 'a+')\n",
    "print(timestamp(), file=logfile_out)\n",
    "print('### PHASE CALCULATIONS ###', file=logfile_out)\n",
    "print('[user] offset added to calculated phases (phi_offset):',phi_offset, file=logfile_out)\n",
    "print('[user] time zeropoint for calculating phases [-1 to use first time] (phi_zeropoint):',phi_zeropoint, file=logfile_out)\n",
    "message1 = '*** Finished calculating phases'\n",
    "print(message1,'\\n', file=logfile_out)\n",
    "logfile_out.close()\n",
    "\n",
    "# end this section gracefully\n",
    "if LDEBUG >= 2: print(df_lc_clean)\n",
    "\n",
    "if LDEBUG >= 0: print(message1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Phase binning\n",
    "Average the phases for a user-selected period into a user-defined number of bins per cycle. The result is a binned light curve for each cycle of the period, and a final \"average\" light curve for the entire phase-folded data set for the period."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################\n",
    "# CUSTOMIZE ANY OF THESE AS NEEDED!!!\n",
    "\n",
    "# step size for phase bins (1/dstep should be an integer)\n",
    "#dstep = 0.10\n",
    "dstep = 0.05\n",
    "###############################################\n",
    "\n",
    "# half of a phase bin width\n",
    "dshift = dstep/2.0\n",
    "# number of phase bins\n",
    "nphibins = int(1.0/dstep)\n",
    "\n",
    "if LDEBUG >= 1: print('nphibins = ',nphibins)\n",
    "\n",
    "# calculate phase bin starting points\n",
    "pbins = [(0.0 + dstep*x)-(dstep/2.0) for x in range(nphibins+1)]\n",
    "pbins[0] = pbins[0]+1.0\n",
    "pbins = [round(i,3) for i in pbins]\n",
    "\n",
    "if LDEBUG >= 2: print(pbins)\n",
    "\n",
    "# calculate centers of phase bins\n",
    "pcbins = [(0.0 + dstep*x) for x in range(nphibins)]\n",
    "pcbins.append(pcbins[0]+1.0)\n",
    "pcbins = [round(i,3) for i in pcbins]\n",
    "\n",
    "if LDEBUG >= 1: print('Bin centers:', pcbins)\n",
    "\n",
    "# write logfile\n",
    "logfile_out = open(logfile_name, 'a+')\n",
    "print(timestamp(), file=logfile_out)\n",
    "print('### PHASE BINNING ###', file=logfile_out)\n",
    "print('[user] step size for phase bins [1/dstep should be an integer] (dstep):',dstep, file=logfile_out)\n",
    "print('half of a phase bin width (dshift):',dshift, file=logfile_out)\n",
    "print('number of phase bins (nphibins):',nphibins, file=logfile_out)\n",
    "print('phase bins (pbins):',pbins, file=logfile_out)\n",
    "print('phase bin centers (pcbins):',pcbins, file=logfile_out)\n",
    "\n",
    "# define new dictionary to hold DataFrames for phase-binned data for all periods\n",
    "df_lc_clean_bin = {}\n",
    "\n",
    "start = time.time()\n",
    "for pidx in range(len(df_pd_peaks.period)):\n",
    "    # sort data into phase bins\n",
    "    spidx = str(pidx)\n",
    "    if pidx < 10: spidx = '0'+spidx\n",
    "    p = np.copy(df_lc_clean['phi'+spidx])\n",
    "    a = np.copy(df_lc_clean['varflux'])\n",
    "    \n",
    "    message1 = 'Processing phased period data '+spidx+' (period = '+str(df_pd_peaks.period[pidx])+')  '+str(time_since(start))\n",
    "    if LDEBUG >= 0: \n",
    "        print(message1)\n",
    "    print(message1, file=logfile_out)\n",
    "    \n",
    "    # define new DataFrame to store phase-binned data for one period\n",
    "    phi_colnames1 = [str(i) for i in pcbins]\n",
    "    phi_colnames2 = [str(i+1) for i in pcbins]\n",
    "    phi_colnames = phi_colnames1 + phi_colnames2[1:]\n",
    "    df_lc_clean_bin_one = pd.DataFrame(columns=phi_colnames)\n",
    "\n",
    "    if LDEBUG >= 2: print(df_lc_clean_bin_one)\n",
    "\n",
    "    # running sum for each phase bin for all cycles (default is 0.0)\n",
    "    allbinsum = [0.0 for i in range(nphibins)]\n",
    "    # running total of data points summed into each phase bin for all cycles (deault is 0)\n",
    "    allbinnum = [0 for i in range(nphibins)]\n",
    "    # average in each phase bin for all cycles (default is Nan)\n",
    "    allbinavg = [np.nan for i in range(nphibins)]\n",
    "    # standard deviation of average in each phase bin for all cycles (default is Nan)\n",
    "    allbinerr = [np.nan for i in range(nphibins)]\n",
    "    \n",
    "    # loop over all phase cycles in input data\n",
    "    for i in range(int(np.floor(max(p)))):\n",
    "        # running sum for each phase bin for one cycle (default is 0.0)\n",
    "        binsum = [0.0 for i in range(nphibins)]\n",
    "        # running total of data points summed into each phase bin for one cycle (default is 0)\n",
    "        binnum = [0 for i in range(nphibins)]\n",
    "        # average in each phase bin for one cycle (default is Nan)\n",
    "        binavg = [np.nan for i in range(nphibins)]\n",
    "    \n",
    "        # create subarrays containing only data in the current cycle\n",
    "        subp = p[(p >= i) & (p < i+1)]\n",
    "        suba = a[(p >= i) & (p < i+1)]    \n",
    "    \n",
    "        if len(subp) > 0:    \n",
    "            # truncate phases to keep just decimal part\n",
    "            subpdec = np.array([mphi(ii) for ii in subp])\n",
    "        \n",
    "            # loop over all phase bins\n",
    "            for j, pcbin in enumerate(pcbins[0:-1]):\n",
    "                # create mask file for reducing length of subarray after data points have been used\n",
    "                mask = np.ones(len(subp), dtype=bool)\n",
    "                mask_flag = False\n",
    "\n",
    "                # sort appropriate data into current phase bin\n",
    "                for k, suba1 in enumerate(suba):                        \n",
    "                    if (mphi(subpdec[k]+dshift) >= pcbin and mphi(subpdec[k]+dshift) < pcbin+dstep):\n",
    "                        allbinsum[j] += suba1\n",
    "                        allbinnum[j] += 1\n",
    "                        binsum[j] += suba1\n",
    "                        binnum[j] += 1\n",
    "                        \n",
    "                        # flag used data points in mask file\n",
    "                        mask[k] = False\n",
    "                        mask_flag = True\n",
    "                \n",
    "                # reduce length of subarrays if any data points were masked\n",
    "                if mask_flag == True:\n",
    "                    subp = subp[mask]\n",
    "                    suba = suba[mask]\n",
    "                    subpdec = subpdec[mask]\n",
    "                \n",
    "                # calculate average value in current bin\n",
    "                if binnum[j] > 0: binavg[j] = binsum[j]/binnum[j]\n",
    "                \n",
    "            # append copy of phase bin averages to provide two cycles of phased data\n",
    "            for ii in range(nphibins):\n",
    "                binavg.append(binavg[ii])\n",
    "            binavg.append(binavg[0])\n",
    "\n",
    "            if LDEBUG >= 2: print(binsum, binnum, binavg)\n",
    "        \n",
    "            # store average phase bin values for this cycle in a row of the dataframe\n",
    "            df_lc_clean_bin_one = df_lc_clean_bin_one.append(pd.Series(binavg, index=df_lc_clean_bin_one.columns), ignore_index=True)\n",
    "        else:\n",
    "            message2 = '***WARNING: no data for phase cycle'+str(i)\n",
    "            if LDEBUG >= 0: \n",
    "                print(message2)\n",
    "            print(message2, file=logfile_out)\n",
    "\n",
    "        if LDEBUG >= 2:\n",
    "            print('\\n===========================================================\\n')\n",
    "\n",
    "    # calculate average and standard deviation values in phase bins for all cycles\n",
    "    for j in range(nphibins):\n",
    "        if allbinnum[j] > 0:\n",
    "            allbinavg[j] = allbinsum[j]/allbinnum[j]\n",
    "        allbinerr[j] = np.std(df_lc_clean_bin_one[phi_colnames[j]])\n",
    "    \n",
    "    # append copy of phase bin averages and standard deviations to provide two cycles of phased data\n",
    "    for ii in range(nphibins):\n",
    "        allbinavg.append(allbinavg[ii])\n",
    "        allbinerr.append(allbinerr[ii])\n",
    "    allbinavg.append(allbinavg[0])        \n",
    "    allbinerr.append(allbinerr[0])        \n",
    "\n",
    "    # store average phase bin values for all cycles in next-to-last row of the DataFrame\n",
    "    # This is the \"average\" light curve for each period\n",
    "    df_lc_clean_bin_one = df_lc_clean_bin_one.append(pd.Series(allbinavg, index=df_lc_clean_bin_one.columns), ignore_index=True)\n",
    "    # store average phase bin standard deviations for all cycles in last row of the DataFrame\n",
    "    df_lc_clean_bin_one = df_lc_clean_bin_one.append(pd.Series(allbinerr, index=df_lc_clean_bin_one.columns), ignore_index=True)\n",
    "\n",
    "    # add the average light curve to the DataFrame for this period phasing\n",
    "    df_lc_clean_bin['phi'+spidx] = df_lc_clean_bin_one\n",
    "\n",
    "# finish logfile entry\n",
    "message3 = '*** Finished phase binning  '+str(time_since(start))\n",
    "print(message3,'\\n', file=logfile_out)\n",
    "logfile_out.close()\n",
    "\n",
    "# end this section gracefully\n",
    "if LDEBUG >= 2:\n",
    "    print('\\n')\n",
    "    print(df_lc_clean_bin_one)\n",
    "\n",
    "if LDEBUG >= 0: print('\\n',message3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# One-dimensional light curve(s)\n",
    "Visualize the data as a 1-d light curve (X = phase, Y = amplitude)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "###############################################\n",
    "# CUSTOMIZE ANY OF THESE AS NEEDED!!!\n",
    "\n",
    "# select period(s) from list of periods defined above (df_pd_peaks.periods)\n",
    "pidx = [11, 8, 1] # orbit, spin, beat\n",
    "#pidx = [11, 8, 1, 0, 2, 3, 4, 5, 6, 7, 9, 10, 12, 13, 14, 15, 16, 17, 18, 19] # orbit, spin, beat, and the rest...\n",
    "\n",
    "# plot axis ranges [xmin, xmax, ymin, ymax]\n",
    "if tidx == 0:\n",
    "    paxvals = [0,2,-50,120]\n",
    "if tidx == 1:\n",
    "    paxvals = [0,2,-15,15]\n",
    "###############################################\n",
    "\n",
    "fcount = 6 # increment output file counter\n",
    "\n",
    "# determine number of light curves to be plotted\n",
    "npanels = len(pidx)\n",
    "\n",
    "# calculate \"half\" of number of panels for use in multicolumn plotting\n",
    "n2 = int(np.ceil(npanels/2))\n",
    "\n",
    "# set up the multipanel plot\n",
    "if npanels <= 4:\n",
    "    # single column of plots\n",
    "    fig, axes = plt.subplots(npanels, 1, sharex=False, figsize=(10,npanels*5), gridspec_kw={'hspace': 0.225})\n",
    "    plxpos = [0.51, 0.09-0.01*(4-npanels)**2]\n",
    "    plypos = [0.02, 0.50]\n",
    "else:\n",
    "    # double column of plots\n",
    "    fig, axes = plt.subplots(n2, 2, sharex=False, sharey=True, figsize=(20,5*n2), gridspec_kw={'hspace': 0.225, 'wspace': 0.06})\n",
    "    if n2 > 3:\n",
    "        plxpos = [0.5125, 0.09+0.0038*(n2-4)]\n",
    "    else:\n",
    "        plxpos = [0.5125, 0.075]\n",
    "    plypos = [0.08, 0.50]\n",
    "    \n",
    "# centered common axis labels\n",
    "fig.text(plxpos[0], plxpos[1], 'Phase', ha='center', size=size_lg)\n",
    "fig.text(plypos[0], plypos[1], 'Flux Amplitude (%)', va='center', rotation='vertical', size=size_lg)\n",
    "\n",
    "# loop over panels\n",
    "for i in range(npanels):    \n",
    "    # convert selected phasing into string for selecting correct DataFrame from dictionary\n",
    "    spidx = str(pidx[i])\n",
    "    if pidx[i] < 10: spidx = '0'+spidx\n",
    "    spidx = 'phi'+spidx\n",
    "\n",
    "    # select unbinned light curve data\n",
    "    x = [mphi(phi) for phi in df_lc_clean[spidx].to_numpy(copy=True)]\n",
    "    y = [amp for amp in df_lc_clean['varflux'].to_numpy(copy=True)]\n",
    "\n",
    "    # double x, y, for plotting 2 cycles\n",
    "    for j in range(len(x)):\n",
    "        x.append(x[j]+1.0)\n",
    "        y.append(y[j])\n",
    "    \n",
    "    # move to current subplot panel\n",
    "    if npanels == 1:\n",
    "        ax = axes\n",
    "    else:\n",
    "        if npanels <= 4:\n",
    "            ax = axes[i]\n",
    "        else:\n",
    "            axflat = axes.flatten()\n",
    "            # hide unused subplot panel if there is an odd number of panels in multicolumn format\n",
    "            if i+1 == npanels and npanels % 2 > 0:\n",
    "                ax = axflat[i+1]\n",
    "                ax.set_visible(False)\n",
    "            ax = axflat[i]\n",
    "    \n",
    "    # plot unbinned light curve\n",
    "    ax.plot(x, norm_self(y), color='k', marker='.', markersize='1', linewidth=0)\n",
    "\n",
    "    # convert column names (phase bin centers) to numbers for plotting\n",
    "    xavg = []\n",
    "    for column in df_lc_clean_bin[spidx].columns:\n",
    "        xavg.append(float(column))\n",
    "\n",
    "    # copy average light curve data and create normalized versions   \n",
    "    yavg = np.copy(df_lc_clean_bin[spidx].iloc[-2])    \n",
    "    yavg_norm = norm_other(yavg,varflux_mean)\n",
    "    yavgerr = np.copy(df_lc_clean_bin[spidx].iloc[-1])\n",
    "    yavgerr_norm = (yavgerr/yavg)*yavg_norm\n",
    "    \n",
    "    # plot averaged light curve stored in last row of DataFrame\n",
    "    ax.errorbar(xavg, yavg_norm, yavgerr_norm, barsabove=False, color='r', marker='o', markersize='8', linewidth=0, elinewidth=1)\n",
    "\n",
    "    # plot decorations\n",
    "    ax.tick_params(labelsize=size_sm)\n",
    "    ax.axis(paxvals)\n",
    "    ax.minorticks_on()\n",
    "    ax.xaxis.set_minor_locator(MultipleLocator(0.05))\n",
    "    #ax.yaxis.set_minor_locator(MultipleLocator(10))\n",
    "    ax.set_title('Period = '+str(df_pd_peaks.period[pidx[i]]), fontsize=size_sm, pad=8)\n",
    "\n",
    "plt.savefig(SAVEPATH+name_string+'_'+strfcount(fcount)+'_lc_folded_'+str(npanels)+'.png',dpi=150, bbox_inches='tight')\n",
    "\n",
    "# write logfile\n",
    "logfile_out = open(logfile_name, 'a+')\n",
    "print(timestamp(), file=logfile_out)\n",
    "print('### ONE-DIMENSIONAL LIGHT CURVE(S) VISUALIZATION ###', file=logfile_out)\n",
    "message1 = '*** Finished plotting folded light curve(s)'\n",
    "print(message1,'\\n', file=logfile_out)\n",
    "logfile_out.close()\n",
    "\n",
    "if LDEBUG >= 0: print(message1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Two-dimensional light curve\n",
    "Visualize the data as a 2-d light curve (X = phase, Y = cycles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "###############################################\n",
    "# CUSTOMIZE ANY OF THESE AS NEEDED!!!\n",
    "\n",
    "# select period from list of periods defined above (df_pd_peaks.periods)\n",
    "pidx =  8 # orbit\n",
    "#pidx = 11 # spin\n",
    "\n",
    "# Select size(s) of the moving window for smoothing (number of cycles, [min, max])\n",
    "# Set min to be the smallest number that eliminates all Nans (white pixels) in the visualization\n",
    "wroll = [ 7, 14]\n",
    "#wroll = [12, 24]\n",
    "\n",
    "# color map name\n",
    "pcmap = 'viridis'\n",
    "#pcmap = 'plasma'\n",
    "#pcmap = 'magma'\n",
    "#pcmap = 'inferno'\n",
    "#pcmap = 'cividis'\n",
    "\n",
    "# skip outputting plot?\n",
    "DO_SKIP=False\n",
    "#DO_SKIP=True\n",
    "###############################################\n",
    "\n",
    "fcount = 7 # increment output file counter\n",
    "\n",
    "# convert selected phasing into string for selecting correct DataFrame from dictionary\n",
    "spidx = str(pidx)\n",
    "if pidx < 10: spidx = '0'+spidx\n",
    "spidx = 'phi'+spidx\n",
    "\n",
    "# select correct DataFrame from dictionary (ignoring last 2 rows, which contain averages and uncertinaties)\n",
    "df_lc_clean_bin_one = df_lc_clean_bin[spidx].iloc[0:-2]\n",
    "        \n",
    "# define the number of rows in the visualization image\n",
    "numrows = len(df_lc_clean_bin_one.index)\n",
    "\n",
    "# aspect ratio (1.0 = 1:1)\n",
    "pasp = 3.0*2.0/numrows\n",
    "\n",
    "# offsets from axis for axis labels (in data units)\n",
    "plxpos = -0.00005*numrows\n",
    "plypos = -0.05\n",
    "\n",
    "# set up the multipanel visualization\n",
    "gs_kw=dict(wspace=0.02)\n",
    "fig, axN = plt.subplots(1,3, sharey=True, figsize=(8.4,7), constrained_layout=True, gridspec_kw=gs_kw)\n",
    "\n",
    "# common axis labels shared by subplot panels\n",
    "#fig.text(plxpos[0], plxpos[1], 'Phase (period = '+str(df_pd_peaks.period[pidx])+' d)', ha='center', size=size_lg)\n",
    "fig.text(0.5, plxpos, 'Phase (period = '+str(df_pd_peaks.period[pidx])+' d)', ha='center', size=size_lg)\n",
    "fig.text(plypos, 0.5, 'Cycles (N='+str(numrows)+')', va='center', rotation='vertical', size=size_lg)\n",
    "\n",
    "# Visualize the data as a 2-dimensional light curve\n",
    "# Three panels show: (1) raw data, (2 & 3) data smoothed by a moving window average (window widths defined above)\n",
    "imethod = 'none'\n",
    "for i in range(0,3):\n",
    "    # make new DataFrame for working in\n",
    "    df_lc_clean_binavg = df_lc_clean_bin_one.copy()\n",
    "    # convert amplitudes to percentage variability\n",
    "    for colname in phi_colnames:\n",
    "        df_lc_clean_binavg[colname] = norm_other(df_lc_clean_bin_one[colname], varflux_mean)\n",
    "    \n",
    "    # apply moving window smoothing\n",
    "    if i >= 1:\n",
    "        for colname in phi_colnames:\n",
    "            df_lc_clean_binavg[colname] = df_lc_clean_binavg[colname].rolling(window=wroll[i-1], min_periods=1, center=True).mean()\n",
    "\n",
    "    # label subplots\n",
    "    if i == 0:\n",
    "        ltitle = 'Raw'   # Missing data (Nans) are visualized in white\n",
    "    else:\n",
    "        ltitle = 'N$_{\\mathrm{roll}}$='+str(wroll[i-1])\n",
    "\n",
    "    # visualize data into subplots\n",
    "    im = axN[i].imshow(df_lc_clean_binavg,aspect=pasp,origin='lower',extent=[0.0,2.0,0,numrows],interpolation=imethod,cmap=pcmap)\n",
    "    \n",
    "    # format and label tickmarks\n",
    "    axN[i].tick_params(labelsize=size_sm, length=5)\n",
    "    axN[i].set_xticks((0, 0.5, 1, 1.5, 2))\n",
    "    axN[i].set_xticklabels(('0','0.5','1','1.5','2'))\n",
    "    axN[i].set_title('('+abclabels[i]+') '+ltitle, fontsize=size_sm, pad=10)\n",
    "\n",
    "    # display color bar\n",
    "    if i == 2:\n",
    "        cbar = fig.colorbar(im, ax=axN[i], shrink=0.8)\n",
    "        cbar.ax.tick_params(labelsize=size_sm)\n",
    "        cbar.set_label('Normalized Amplitude (%)', fontsize=size_lg, labelpad=28, rotation=270)\n",
    "\n",
    "# output the visualization\n",
    "if not DO_SKIP:\n",
    "    fig.savefig(SAVEPATH+name_string+'_'+strfcount(fcount)+'_lc_trailed_'+str(df_pd_peaks.period[pidx])+'.png',dpi=150, bbox_inches='tight')\n",
    "\n",
    "# write logfile\n",
    "logfile_out = open(logfile_name, 'a+')\n",
    "print(timestamp(), file=logfile_out)\n",
    "print('### TWO-DIMENSIONAL LIGHT CURVE VISUALIZATION ###', file=logfile_out)\n",
    "message1 = '*** Finished displaying 2-dimensional trailed light curves'\n",
    "print(message1,'\\n', file=logfile_out)\n",
    "logfile_out.close()\n",
    "\n",
    "# end this section gracefully\n",
    "if LDEBUG >= 0: print(message1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Folded two-dimensional light curve\n",
    "Visualize the data as a 2-d light curve folded over a specified number of cycles (X = phase, Y = cycles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "###############################################\n",
    "# CUSTOMIZE ANY OF THESE AS NEEDED!!!\n",
    "\n",
    "# number of cycles to fold over\n",
    "nfold = 36\n",
    "\n",
    "# select period from list of periods defined above (df_pd_peaks.periods)\n",
    "pidx =  8 # orbit\n",
    "\n",
    "# Select size(s) of the moving window for smoothing (number of cycles)\n",
    "wroll = 5\n",
    "\n",
    "# color map name\n",
    "pcmap = 'viridis'\n",
    "#pcmap = 'plasma'\n",
    "#pcmap = 'magma'\n",
    "#pcmap = 'inferno'\n",
    "#pcmap = 'cividis'\n",
    "\n",
    "# aspect ratio (1.0 = 1:1)\n",
    "pasp = 0.02\n",
    "\n",
    "# offsets from axis for axis labels (in data units)\n",
    "plxpos = -10\n",
    "plypos = -0.25\n",
    "\n",
    "# scale factor for axis tickmarks\n",
    "ptscale = 2.0\n",
    "\n",
    "# skip outputting plot?\n",
    "DO_SKIP=False\n",
    "#DO_SKIP=True\n",
    "###############################################\n",
    "\n",
    "fcount = 8 # increment output file counter\n",
    "\n",
    "# convert selected phasing into string for selecting correct DataFrame from dictionary\n",
    "spidx = str(pidx)\n",
    "if pidx < 10: spidx = '0'+spidx\n",
    "spidx = 'phi'+spidx\n",
    "\n",
    "# select correct DataFrame from dictionary (ignoring last 2 rows, which contain averages and uncertinaties)\n",
    "df_lc_clean_bin_nans = df_lc_clean_bin[spidx].iloc[0:-2]\n",
    "df_lc_clean_bin_zero = df_lc_clean_bin_nans.fillna(0)\n",
    "\n",
    "# define the number of rows in the visualization image\n",
    "numrows = len(df_lc_clean_bin_one.index)\n",
    "\n",
    "df_lc_clean_binfold = pd.DataFrame(columns=phi_colnames)\n",
    "\n",
    "for i in range(nfold):\n",
    "    fold_one = 0.0 * df_lc_clean_bin_zero.iloc[0]\n",
    "    fold_count = 0 * df_lc_clean_bin_zero.iloc[0]\n",
    "\n",
    "    for j in range(i,numrows,nfold):\n",
    "        fold_one += df_lc_clean_bin_zero.iloc[j]\n",
    "        for k in range(len(phi_colnames)):\n",
    "            if not np.isnan(df_lc_clean_bin_nans[phi_colnames[k]].iloc[j]):\n",
    "                fold_count[k] += 1\n",
    "\n",
    "    fold_avg = np.divide(fold_one, fold_count)\n",
    "    \n",
    "    df_lc_clean_binfold = df_lc_clean_binfold.append(pd.Series(fold_avg, index=df_lc_clean_binfold.columns), ignore_index=True)\n",
    "\n",
    "# Visualize the data as a 2-dimensional light curve\n",
    "# make new DataFrame for working in\n",
    "#df_lc_clean_binfold_norm = df_lc_clean_binfold.copy()\n",
    "df_lc_clean_binfold_norm = pd.concat([df_lc_clean_binfold.copy(), df_lc_clean_binfold.copy()], ignore_index=True, sort=False)\n",
    "\n",
    "#for c,d in enumerate(df_lc_clean_binfold_norm[phi_colnames[1]]): print(c,d)\n",
    "    \n",
    "# convert amplitudes to percentage variability\n",
    "for colname in phi_colnames:\n",
    "    df_lc_clean_binfold_norm[colname] = norm_other(df_lc_clean_binfold_norm[colname], varflux_mean)    \n",
    "\n",
    "# apply moving window smoothing\n",
    "if wroll > 1:\n",
    "    df_lc_clean_binfold_norm2 = pd.DataFrame(columns=phi_colnames)\n",
    "    for colname in phi_colnames:\n",
    "        df_lc_clean_binfold_norm2[colname] = df_lc_clean_binfold_norm[colname].rolling(window=wroll, win_type='boxcar', min_periods=1, center=True).mean()\n",
    "        df_lc_clean_binfold_norm2.loc[0] = df_lc_clean_binfold_norm2.loc[nfold] \n",
    "        df_lc_clean_binfold_norm2.loc[1] = df_lc_clean_binfold_norm2.loc[nfold+1] \n",
    "        #df_lc_clean_binfold_norm2.loc[nfold-1] = df_lc_clean_binfold_norm2.loc[nfold-1]*0.0 \n",
    "        df_lc_clean_binfold_norm2.loc[2*nfold-1] = df_lc_clean_binfold_norm2.loc[nfold-1] \n",
    "        df_lc_clean_binfold_norm2.loc[2*nfold-2] = df_lc_clean_binfold_norm2.loc[nfold-2] \n",
    "        #df_lc_clean_binfold_norm2.loc[2*nfold-3] = df_lc_clean_binfold_norm2.loc[nfold-3] \n",
    "        \n",
    "# label plot\n",
    "ltitle = 'N$_{\\mathrm{fold}}$='+str(nfold)+', N$_{\\mathrm{roll}}$='+str(wroll)\n",
    "\n",
    "# visualize data\n",
    "imethod = 'none'\n",
    "im = plt.imshow(df_lc_clean_binfold_norm2,aspect=pasp,origin='lower',extent=[0.0,2.0,0,2*nfold],interpolation=imethod,cmap=pcmap)\n",
    "\n",
    "# common axis labels shared by subplot panels\n",
    "plt.text(1.0, plxpos, 'Phase (period = '+str(df_pd_peaks.period[pidx])+' d)', horizontalalignment='center', size=size_lg)\n",
    "plt.text(plypos, nfold, 'Folded Cycles', verticalalignment='center', rotation='vertical', size=size_lg)\n",
    "plt.text(1.0, 1.05*2*nfold, ltitle, horizontalalignment='center', size=size_lg)\n",
    "\n",
    "# format and label tickmarks\n",
    "#ax.tick_params(labelsize=size_sm, length=5)\n",
    "plt.xticks((0, 0.5, 1, 1.5, 2))\n",
    "plt.minorticks_on()\n",
    "plt.tick_params(axis='both', labelsize=size_sm)\n",
    "plt.tick_params(axis='x', which='major', length=ptscale*plt.rcParams[\"xtick.major.size\"])\n",
    "plt.tick_params(axis='x', which='minor', length=ptscale*plt.rcParams[\"xtick.minor.size\"])\n",
    "plt.tick_params(axis='y', which='major', length=ptscale*plt.rcParams[\"ytick.major.size\"])\n",
    "plt.tick_params(axis='y', which='minor', length=ptscale*plt.rcParams[\"ytick.minor.size\"])\n",
    "\n",
    "# display color bar\n",
    "cbar = plt.colorbar(im, shrink=0.8)\n",
    "cbar.ax.tick_params(labelsize=size_sm)\n",
    "cbar.set_label('Normalized Amplitude (%)', fontsize=size_lg, labelpad=28, rotation=270)\n",
    "\n",
    "# output the visualization\n",
    "if not DO_SKIP:\n",
    "    plt.savefig(SAVEPATH+name_string+'_'+strfcount(fcount)+'_lc_trailed_folded_'+str(df_pd_peaks.period[pidx])+'.png',dpi=150, bbox_inches='tight')\n",
    "\n",
    "# write logfile\n",
    "logfile_out = open(logfile_name, 'a+')\n",
    "print(timestamp(), file=logfile_out)\n",
    "print('### FOLDED TWO-DIMENSIONAL LIGHT CURVE VISUALIZATION ###', file=logfile_out)\n",
    "message1 = '*** Finished displaying folded 2-dimensional trailed light curves'\n",
    "print(message1,'\\n', file=logfile_out)\n",
    "logfile_out.close()\n",
    "\n",
    "# end this section gracefully\n",
    "if LDEBUG >= 0: print(message1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# REFERENCES\n",
    "\n",
    "## General\n",
    "\n",
    "1. Command continuation on multiple lines:\n",
    "    - https://stackoverflow.com/questions/17076710/python-split-statement-into-multiple-lines\n",
    "1. Comparison of interpolation methods:\n",
    "    - https://matplotlib.org/3.1.0/gallery/images_contours_and_fields/interpolation_methods.html#sphx-glr-gallery-images-contours-and-fields-interpolation-methods-py\n",
    "1. Find missing elements in integer sequence:\n",
    "    - https://stackoverflow.com/questions/16974047/efficient-way-to-find-missing-elements-in-an-integer-sequence\n",
    "1. Gaussian fit:\n",
    "    - https://stackoverflow.com/questions/19206332/gaussian-fit-for-python\n",
    "1. Multiprocessing:\n",
    "    - https://www.praetorian.com/blog/multi-core-and-distributed-programming-in-python?edition=2019\n",
    "1. Replace NaNs with zeros:\n",
    "    - https://stackoverflow.com/questions/13295735/how-can-i-replace-all-the-nan-values-with-zeros-in-a-column-of-a-pandas-datafra\n",
    "1. Robust random numbers (use SystemRandom):\n",
    "    - https://smallbusiness.chron.com/randomize-list-python-26724.html\n",
    "\n",
    "\n",
    "## Periodogram\n",
    "\n",
    "1. Cumulative distribution plot:\n",
    "    - https://stackoverflow.com/questions/15408371/cumulative-distribution-plots-python\n",
    "1. Confidence level examples:\n",
    "    - https://www-zeuthen.desy.de/students/2017/Summerstudents2017/reports/GiulianaNoto.pdf (Section 3.4)\n",
    "1. False Alarm Probability (FAP)\n",
    "    - https://arxiv.org/pdf/1703.09824.pdf (Bootstrapping the FAP; Section 7.4.2)\n",
    "    - https://mail.python.org/pipermail//astropy/2016-June/003937.html\n",
    "1. Non-Uniform Nyquist Limit:\n",
    "    - https://arxiv.org/pdf/1703.09824.pdf (Section 4.1.2)\n",
    "1. Normalization:\n",
    "    - https://github.com/scipy/scipy/issues/2162\n",
    "1. Number of periodogram frequencies:\n",
    "    - https://arxiv.org/pdf/1703.09824.pdf (Section 7.1)\n",
    "1. \"Understanding the Lomb-Scargle Periodogram\" by J. VanderPlas\n",
    "    - https://arxiv.org/abs/1703.09824\n",
    "\n",
    "\n",
    "## Plotting in Python\n",
    "\n",
    "1. Annotation (matplotlib.pyplot.annotate):\n",
    "    - https://matplotlib.org/3.1.1/api/_as_gen/matplotlib.pyplot.annotate.html\n",
    "1. Axis labels:\n",
    "    - https://stackoverflow.com/questions/33283601/manually-defined-axis-labels-for-matplotlib-imshow/33283892\n",
    "1. Colorbars:\n",
    "    - https://stackoverflow.com/questions/18266642/multiple-imshow-subplots-each-with-colorbar\n",
    "    - https://stackoverflow.com/questions/40184696/change-fontsize-of-colorbars-in-matplotlib\n",
    "1. Colormaps:\n",
    "    - https://matplotlib.org/3.1.0/tutorials/colors/colormaps.html\n",
    "1. Error bars:\n",
    "    - https://matplotlib.org/3.1.0/api/_as_gen/matplotlib.pyplot.errorbar.html\n",
    "1. Make a subplot invisible:\n",
    "    - https://stackoverflow.com/questions/14694501/delete-a-subplot\n",
    "1. Multipanel plot using a FOR loop\n",
    "    - https://stackoverflow.com/questions/42845887/how-to-run-a-smart-loop-for-subplots-in-python\n",
    "1. Spacing of subplots:\n",
    "    - https://stackoverflow.com/questions/37864735/matplotlib-and-ipython-notebook-displaying-exactly-the-figure-that-will-be-save\n",
    "    - https://stackoverflow.com/questions/6541123/improve-subplot-size-spacing-with-many-subplots-in-matplotlib    \n",
    "1. Tick font size in subplots:\n",
    "    - https://stackoverflow.com/questions/38369188/set-size-of-ticks-in-all-subplots\n",
    "1. Tick parameters:\n",
    "    - https://matplotlib.org/3.1.0/api/_as_gen/matplotlib.axes.Axes.tick_params.html\n",
    "1. Titles:\n",
    "    - https://stackoverflow.com/questions/8248467/matplotlib-tight-layout-doesnt-take-into-account-figure-suptitle\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
