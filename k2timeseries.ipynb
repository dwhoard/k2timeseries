{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variable stars from the NASA K2 mission\n",
    "\n",
    "[Kepler & K2](https://keplerscience.arc.nasa.gov) provide long-time-baseline, high-precision photometry for exoplanet and astrophysics research.\n",
    "\n",
    "This Notebook contains code written and adapted by D. W. Hoard for processing, cleaning, performing time series analysis, and visualizing K2 time series data for variable stars.\n",
    "\n",
    "\n",
    "# Workflow for processing K2 time series (light curve) data\n",
    "\n",
    "I retrieved the raw \\*\\_llc.fits data files (e.g., ktwo212218649-c16_llc.fits) from the [MAST archive](https://mast.stsci.edu/). I have already pre-processed them into \\*\\mast-kepconvert.txt data files (e.g., EPIC_212218649_sap_mast-kepconvert.txt) using the separate Python tasks k2sc and kepconvert. For completeness, the basic process for installing those packages and performing the pre-processing is decribed below. **BUT THIS NOTEBOOK USES THE PRE-PROCESSED FILES AS INPUT.**\n",
    "\n",
    "\n",
    "## Documentation:\n",
    "\n",
    "- K2SC\n",
    "https://archive.stsci.edu/prepds/k2sc/\n",
    "\n",
    "\n",
    "- KEPCONVERT\n",
    "https://pyke.keplerscience.org/tasks/kepconvert.html\n",
    "\n",
    "\n",
    "## Installation and use of pre-processing packages\n",
    "\n",
    "k2sc: Install from [GitHub](https://github.com/OxES/k2sc).\n",
    "\n",
    "kepconvert: Installed as part of the [PyKE Python package](https://pyke.keplerscience.org/index.html)\n",
    "\n",
    "If you have installed Anaconda, then k2sc and kepconvert can be installed as follows:\n",
    "~~~~\n",
    "# Install PyKE (for kepconvert)\n",
    "pip install msgpack --upgrade\n",
    "pip install argparse --upgrade\n",
    "pip install pyketools --upgrade\n",
    "\n",
    "# Install K2SC prerequisites:\n",
    "pip install numpy --upgrade\n",
    "pip install scipy --upgrade\n",
    "pip install astropy --upgrade\n",
    "pip install george --upgrade\n",
    "\n",
    "# OPTIONAL (for parallelization):\n",
    "pip install mpia4py --upgrade\n",
    "\n",
    "# Install K2SC\n",
    "cd ~/Applications/  # or installation directory of your choice; ensure that $DIRECTORY/k2sc/bin is in your PATH\n",
    "git clone https://github.com/OxES/k2sc.git\n",
    "cd k2sc\n",
    "python setup.py install --user\n",
    "~~~~\n",
    "\n",
    "### Sample Python command line sequence for pre-processing of input data\n",
    "\n",
    "~~~~\n",
    "### Definitions  \n",
    "# Target K2 (EPIC) ID number  \n",
    "k2id='212218649'  \n",
    "# Campaign number  \n",
    "camid='16'  \n",
    "\n",
    "### Automatically construct additional definitions\n",
    "iname='ktwo'${k2id}'-c'${camid}\n",
    "lcf=${iname}'_llc.fits'\n",
    "\n",
    "### Run k2sc for SAP data\n",
    "fltype='sap'\n",
    "k2sc ${lcf} --campaign ${camid} --flux-type ${fltype} --logfile k2sc_${fltype}.log\n",
    "\n",
    "### Export SAP light curve into ascii file\n",
    "scf0='EPIC_'${k2id}'_mast.fits'\n",
    "scf='EPIC_'${k2id}'_'${fltype}'_mast.fits'\n",
    "mv ${scf0} ${scf}\n",
    "kepconvert ${scf} fits2asc --columns TIME,CADENCE,QUALITY,X,Y,FLUX,ERROR,MFLAGS,TRTIME,TRPOSI --overwrite --verbose --logfile kepconvert_${fltype}.log\n",
    "\n",
    "### Run k2sc for PDC data\n",
    "fltype='pdc'\n",
    "k2sc ${lcf} --campaign ${camid} --flux-type ${fltype} --logfile k2sc_${fltype}.log\n",
    "\n",
    "### Export PDC light curve into ascii file\n",
    "scf0='EPIC_'${k2id}'_mast.fits'\n",
    "scf='EPIC_'${k2id}'_'${fltype}'_mast.fits'\n",
    "mv ${scf0} ${scf}\n",
    "kepconvert ${scf} fits2asc --columns TIME,CADENCE,QUALITY,X,Y,FLUX,ERROR,MFLAGS,TRTIME,TRPOSI --overwrite --verbose --logfile kepconvert_${fltype}.log\n",
    "~~~~\n",
    "\n",
    "\n",
    "## SAP vs. PDC data?\n",
    "\n",
    "In short, using the SAP data is preferred for these purposes because some of the artifact removal applied in the PDC pipeline (which assumes that the stars are intrsinically non-variable, other than brief exoplanet transits) can remove actual variability from my targets-of-interest.\n",
    "\n",
    "See https://keplerscience.arc.nasa.gov/pipeline.html\n",
    "\n",
    "\n",
    "## Important note\n",
    "The k2sc README file is wrong about output column contents. FLUX is a copy of the input (raw) flux, TRTIME is the time-dependent (variability) component of model, and TRPOSI is the position-dependent (systematics) component of model. The calculations performed below are correct."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Import general purpose packages used throughout the workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Set global user-defined parameters\n",
    "CUSTOMIZE ANY OF THESE AS NEEDED!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################\n",
    "# TARGET AND DATA IDENTIFIERS\n",
    "# list of unique target identifier string(s)\n",
    "tname = ['212218649', '212223804', '212213538']\n",
    "# index of target identifier to use (starts at zero)\n",
    "tidx = 0\n",
    "\n",
    "# select the input data type (SAP or PDC) - see note above\n",
    "dtype = 'sap'\n",
    "#dtype = 'pdc'\n",
    "\n",
    "# GENERAL CONFIGURATION\n",
    "# path for saving output files\n",
    "SAVEPATH = './output/'\n",
    "\n",
    "# verbosity level of debugging/info comments\n",
    "# (-1 = none, 0 = crucial only, 1 = some, 2 = all)\n",
    "LDEBUG = 1\n",
    "###############################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Set global default parameters\n",
    "ALMOST NEVER NEED TO CHANGE THESE!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define part of the output file names base don target identifier and data type\n",
    "name_string = tname[tidx]+'_'+dtype\n",
    "\n",
    "# default dimensions for visualization output files, if not specified in individual \"fig\" setups (inches)\n",
    "PSIZE_DEFAULT = (12,6)\n",
    "plt.rcParams[\"figure.figsize\"] = PSIZE_DEFAULT\n",
    "\n",
    "# optional addition to input path if non-variable \"check\" data is stored in a separate directory\n",
    "import os.path\n",
    "INPUTPATH = ''\n",
    "if tidx > 0 and os.path.exists('nearby_stars'):\n",
    "    INPUTPATH = 'nearby_stars/ktwo'+tname[tidx]+'/'\n",
    "                             \n",
    "# fontsize for (1) tick labels, (2) axis labels\n",
    "pfontsize1 = '14'\n",
    "pfontsize2 = '20'\n",
    "\n",
    "# make array of alphabet letters for general use later\n",
    "abclabels = list(string.ascii_lowercase)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Define global functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert file counter to string for creating output file names\n",
    "def strfcount(fcount):\n",
    "    sfcount = str(fcount)\n",
    "    if fcount < 10:\n",
    "        sfcount = '0'+sfcount\n",
    "    return sfcount"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Read unfolded (\"raw\") light curve data into a DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# create file handler in read mode \n",
    "file_handler = open(INPUTPATH+\"EPIC_\"+name_string+\"_mast-kepconvert.txt\", \"r\") \n",
    "\n",
    "# define column names and convert data to DataFrame\n",
    "dfcols  = ['time', 'cadence', 'quality', 'xpos', 'ypos', 'rawflux', 'fluxerr', 'mflags', 'trtime', 'trposi']\n",
    "df_lc = ( pd.read_csv(file_handler, sep=' ', usecols=(0,1,2,3,4,5,6,7,8,9), header=None, skiprows=1, names=dfcols, \n",
    "        dtype={dfcols[0]:'float64',dfcols[1]:'int64',dfcols[2]:'int64',dfcols[3]:'float64',dfcols[4]:'float64',\n",
    "        dfcols[5]:'float64',dfcols[6]:'float64',dfcols[7]:'int64',dfcols[8]:'float64',dfcols[9]:'float64'}) )\n",
    "\n",
    "# close the file handler \n",
    "file_handler.close() \n",
    "\n",
    "# remove rows with NaNs in important columns\n",
    "df_lc.dropna(subset=['time', 'rawflux', 'fluxerr'], inplace=True)\n",
    "\n",
    "# trtime = time-dependent (variability) component of K2SC model\n",
    "# trposi = position-dependent (systematics) component of K2SC model\n",
    "\n",
    "# this is the raw flux with position-dependent artifacts removed (i.e., time-dependent variability only)\n",
    "df_lc['varflux'] = df_lc['rawflux'] - df_lc['trposi'] + np.mean(df_lc['trposi'])\n",
    "\n",
    "# this is the raw flux with time variability removed (i.e., only position-dependent variability is present)\n",
    "df_lc['posflux'] = df_lc['rawflux'] - df_lc['trtime'] + np.mean(df_lc['trtime'])\n",
    "\n",
    "# this is the residual flux with both components removed\n",
    "df_lc['resflux'] = df_lc['rawflux'] - df_lc['trtime'] + np.mean(df_lc['trtime']) - df_lc['trposi'] + np.mean(df_lc['trposi'])\n",
    "\n",
    "# calulate Kepler magnitude from time variability series (2016AJ....152....5D)\n",
    "df_lc['kp2mag'] = 12.0 + (np.log10(df_lc['varflux']/1.74e5) / (-0.4))\n",
    "\n",
    "# remove rows with poor Quality values or masked values (bad values defined in K2 documentation)\n",
    "df_lc_clean = df_lc[(df_lc.quality < 2.0**19) & (df_lc.mflags == 0)].copy()\n",
    "df_lc_dirty_only = df_lc[(df_lc.quality >= 2.0**19) | (df_lc.mflags > 0)].copy()\n",
    "\n",
    "if LDEBUG >=2: \n",
    "    pd.options.display.float_format = '{:f}'.format\n",
    "    print(df_lc_clean)\n",
    "    print(df_lc_dirty_only)\n",
    "if LDEBUG >=0:\n",
    "    print('*** Finished loading and cleaning input data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# This is a placeholder cell.\n",
    "Code to visualize the raw and cleaned data will go here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fcount = 1 # increment output file counter\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Set up for Lomb-Scargle periodogram calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################\n",
    "# CUSTOMIZE ANY OF THESE AS NEEDED!!!\n",
    "\n",
    "# override automatic calculation of maximum frequency\n",
    "# (use -1 to ignore this setting)\n",
    "user_fmax = -1\n",
    "###############################################\n",
    "\n",
    "# slice out the periodogram input data\n",
    "x = df_lc_clean.time.to_numpy()\n",
    "N = len(x)\n",
    "\n",
    "# store these values for using later\n",
    "varflux_mean = np.mean(df_lc_clean.varflux) # mean of the cleaned varflux\n",
    "varflux_std  = np.std(df_lc_clean.varflux)  # standard deviation of the cleaned varflux\n",
    "\n",
    "if LDEBUG >= 2: print('varflux_mean, varflux_std: ', varflux_mean, varflux_std)\n",
    "\n",
    "# normalize amplitude data to mean = 0, variance = 1\n",
    "y = (df_lc_clean.varflux.to_numpy() - varflux_mean) / varflux_std\n",
    "\n",
    "if LDEBUG >= 1: print('Mean, Variance (expect 0, 1):', abs(round(np.mean(y),3)), round(np.var(y),3))\n",
    "\n",
    "# define frequency parameters for periodogram calculation\n",
    "delta_x = [x[i+1] - x[i] for i in range(N-2)]   # list of time differences between points\n",
    "\n",
    "if LDEBUG >= 2: print(min(delta_x), max(delta_x), np.mean(delta_x))\n",
    "\n",
    "# Calculate minimum valid frequency for periodogram (fmin)\n",
    "# fmax corresponds to 1 cycle over the entire duration of the data set\n",
    "# fmin = 1/T, where T = length of data set (time difference between first and last points)\n",
    "T = x[-1]-x[0]\n",
    "fmin = (1.0/T)\n",
    "\n",
    "# Calculate maximum valid frequency for periodogram (fmax)\n",
    "# fmax is based on the Nyquist criterion using the smallest separation of data points as an estimate\n",
    "# (or, to make calculating the periodogram more manageable, set fmax to a smaller frequency that \n",
    "# makes sense for the expected periods)\n",
    "if user_fmax < 0:\n",
    "    fmax = (1.0/(2.0*min(delta_x)))\n",
    "else: \n",
    "    if LDEBUG >= 0: print('User override: maximum frequncy set to', user_fmax)\n",
    "    fmax = user_fmax\n",
    "    \n",
    "# Number of frequencies for which to calculate the periodogram (see References)\n",
    "Nf = int(5.0 * T * fmax)\n",
    "\n",
    "if LDEBUG >= 2: print('T, fmin, fmax, 1/fmin, 1/fmax, Nf: ', T, fmin, fmax, 1/fmin, 1/fmax, Nf)\n",
    "if LDEBUG >= 1:\n",
    "    print('Length of data set:', T)\n",
    "    print('Min/Max frequency & period:', fmin, fmax, 1/fmin, 1/fmax)\n",
    "    print('Number of frequencies in periodogram:', Nf)\n",
    "\n",
    "# scale factor to convert frequencies to angular frequencies\n",
    "scfac = 2.0*np.pi\n",
    "\n",
    "# create an array of linearly spaced frequencies\n",
    "f = [(fmin + i*(fmax-fmin)/Nf) for i in range(Nf)]\n",
    "w = [(fmin + i*(fmax-fmin)/Nf)*scfac for i in range(Nf)]\n",
    "\n",
    "if LDEBUG >= 0: \n",
    "    print('\\n')\n",
    "    print('*** Finished setting up for Lomb-Scargle periodogram')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Calculate Lomb-Scargle periodogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fcount = 2 # increment output file counter\n",
    "\n",
    "# import specific packages needed here\n",
    "from scipy import signal\n",
    "from matplotlib.ticker import MultipleLocator\n",
    "\n",
    "# calculate periodogram\n",
    "# normalization = multiply periodogram amplitude by (2 / len(x))\n",
    "npgram = signal.lombscargle(x, y, w, normalize=True)\n",
    "\n",
    "# visualize periodogram\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(f, npgram)\n",
    "\n",
    "# plot decorations\n",
    "y_minor_tick = 0.005\n",
    "ax.axis([0,np.ceil(max(f)),-2*y_minor_tick,max(npgram)+2*y_minor_tick])\n",
    "ax.minorticks_on()\n",
    "ax.yaxis.set_minor_locator(MultipleLocator(y_minor_tick))\n",
    "plt.xticks(fontsize=pfontsize1)\n",
    "plt.yticks(fontsize=pfontsize1)\n",
    "plt.xlabel('Frequency (d$^{-1}$)',fontsize=pfontsize2, labelpad=10)\n",
    "plt.ylabel('Normalized Amplitude', fontsize=pfontsize2, labelpad=20)\n",
    "\n",
    "# output visualization\n",
    "plt.savefig(SAVEPATH+name_string+'_'+strfcount(fcount)+'_LSperiodogram.png',dpi=150, bbox_inches='tight')\n",
    "\n",
    "if LDEBUG >= 0:\n",
    "    print('*** Finished calculating L-S periodogram')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Bootstrap the false alarm probability (FAP)\n",
    "\n",
    "The FAP provides a means of evaluating the likelihood that a given peak in the periodogram is real (i.e., the inverse of the probability that the peak could have arisen by chance). The FAP is determined by performing a Monte Carlo simulation with many trials. In each trial, a new periodogram is calculated, with the input amplitude data randomly scrambled to different time values in each trial. The cumulative distribution of maximum peak heights in the trials results yields the FAP.\n",
    "\n",
    "Because of the large number of trials, this step can be time consuming. The code has been written to allow for parallel processing on multiple cores. This step can also be skipped entirely, in which case a file of previouly generated peak heights is read in the next step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################\n",
    "# CUSTOMIZE ANY OF THESE AS NEEDED!!!\n",
    "\n",
    "# select whether (True) or not (False) to run the Monte Carlo simulation, which is time-consuming\n",
    "do_bootstrap = False\n",
    "#do_bootstrap = True\n",
    "\n",
    "# define parameters for FAP Monte Carlo simulation\n",
    "\n",
    "# NOTE: even if do_bootstrap=False, the value of Np calculated below from the\n",
    "# value of fap_desired is used to select the FAP data file to read in the next cell.\n",
    "\n",
    "# 0.01 = 1% false positive rate\n",
    "fap_desired = 0.0001  # = 0.01% Np = 100,000\n",
    "#fap_desired = 0.0002  # = 0.02% Np =  50,000\n",
    "#fap_desired = 0.0005  # = 0.05% Np =  20,000\n",
    "#fap_desired = 0.001   # = 0.1%  Np =  10,000\n",
    "\n",
    "# number of processor cores to use (should be <= actual number of available cores!)\n",
    "# current maximum = 4 but can be extended by adding more \"proc\" statements below\n",
    "nproc = 4\n",
    "###############################################\n",
    "\n",
    "fcount = 3 # increment output file counter\n",
    "\n",
    "# calculate number of Monte Carlo trials based on desired precision of FAP probabilities\n",
    "Np = int(10/fap_desired)\n",
    "\n",
    "if do_bootstrap == True:\n",
    "    # import specific packages\n",
    "    from random import SystemRandom\n",
    "    from random import shuffle\n",
    "    import copy\n",
    "    from multiprocessing import Process, Queue\n",
    "\n",
    "    # function for running on multiple processor cores\n",
    "    def boostrap_func(x, y, w):\n",
    "        # create shuffled version of amplitude data (y)\n",
    "        y_trial = copy.copy(y)\n",
    "        r.shuffle(y_trial)\n",
    "        # calcualte trial periodogram using shuffled data\n",
    "        npgram_trial = signal.lombscargle(x, y_trial, w, normalize=True)\n",
    "        # save maximum peak height of trial periodogram as output of trial\n",
    "        q.put(max(npgram_trial))\n",
    "\n",
    "    # number of trials to perform with each processor core\n",
    "    Np_multi = int(Np/nproc)\n",
    "    \n",
    "    if LDEBUG >= 2: print(fap_desired, Np, Np_multi)\n",
    "\n",
    "    # implement OS-level randomization (better than python internal)\n",
    "    r = SystemRandom()\n",
    "\n",
    "    # run Monte Carlo simulation\n",
    "    max_npgram_trial = []\n",
    "    for j in range(Np_multi):\n",
    "        # create a queue to share results\n",
    "        q = Queue()\n",
    "        # create nproc sub-processes to do the work\n",
    "        if (nproc >= 1):\n",
    "            proc1 = Process(target=boostrap_func, args=(x, y, w))\n",
    "            proc1.start()\n",
    "        if (nproc >= 2):\n",
    "            proc2 = Process(target=boostrap_func, args=(x, y, w))\n",
    "            proc2.start()\n",
    "        if (nproc >= 3):\n",
    "            proc3 = Process(target=boostrap_func, args=(x, y, w))\n",
    "            proc3.start()\n",
    "        if (nproc >= 4):\n",
    "            proc4 = Process(target=boostrap_func, args=(x, y, w))\n",
    "            proc4.start()\n",
    "        \n",
    "        results = []\n",
    "        # grab values from the queue, one for each process\n",
    "        for i in range(nproc):\n",
    "            # set block=True to block until we get a result\n",
    "            results.append(q.get(True))\n",
    "    \n",
    "        # join results from invidiual processors\n",
    "        if (nproc >= 1):\n",
    "            proc1.join()\n",
    "        if (nproc >= 2):\n",
    "            proc2.join()\n",
    "        if (nproc >= 3):\n",
    "            proc3.join()\n",
    "        if (nproc >= 4):\n",
    "            proc4.join()             \n",
    "    \n",
    "        if LDEBUG >= 2: print(j, results)\n",
    "\n",
    "        # create list of trial periodogram maximum peak heights for FAP statistics\n",
    "        for k in range(nproc):\n",
    "            max_npgram_trial.append(results[k])\n",
    "        \n",
    "        if LDEBUG >= 1: \n",
    "            if (j+1) <= 20 or (j+1) % 100 == 0:\n",
    "                print(j+1, (j+1)*nproc, len(max_npgram_trial), max_npgram_trial[j], min(max_npgram_trial), max(max_npgram_trial))\n",
    "\n",
    "    if LDEBUG >= 1: print(j+1, (j+1)*nproc, len(max_npgram_trial), max_npgram_trial[j], min(max_npgram_trial), max(max_npgram_trial))\n",
    "    if LDEBUG >= 0: print('*** Finished bootstrapping iterations')\n",
    "\n",
    "    # write bootstrap results to output file\n",
    "    fname1 = SAVEPATH+name_string+'_'+strfcount(fcount)+'_bootstrap_max_npgram_trials_'+str(Np)+'.txt'\n",
    "    outfile = open(fname1, 'w')\n",
    "    for row in max_npgram_trial:\n",
    "        print(str(row), file=outfile)\n",
    "    outfile.close()\n",
    "\n",
    "    if LDEBUG >= 0:\n",
    "        print('*** Finished writing file of bootstrap results')\n",
    "else:\n",
    "    if LDEBUG >= 0: \n",
    "        print('*** Skipping bootstrap simulation - read file of previous bootstrap results instead')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Determine FAP confidence levels from bootstrap results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "###############################################\n",
    "# CUSTOMIZE ANY OF THESE AS NEEDED!!!\n",
    "\n",
    "# read previous bootstrap output file instead of re-running bootstrap procedure?\n",
    "#read_fap = False\n",
    "read_fap = True\n",
    "\n",
    "# define histogram bins for peak heights\n",
    "minbin = 0.002   # should be less than the actual minimum peak height in FAP M.C. simulation\n",
    "maxbin = 0.015   # should be greater than the actual maximum peak height in FAP M.C. simulation\n",
    "nhbins = 1000    # a reasonably large number, but avoid having bins with <10 counts\n",
    "\n",
    "# desired FAP confidence levels to evaluate\n",
    "fap_levels = ( [\n",
    "0.9973,   # 3-sigma confidence\n",
    "0.9545,   # 2-sigma confidence\n",
    "0.90,     # 90% confidence\n",
    "0.6827,   # 1-sigma confidence\n",
    "0.50      # 50% confidence\n",
    "])\n",
    "###############################################\n",
    "\n",
    "fcount = 4 # increment output file counter\n",
    "\n",
    "# read FAP Monte Carlo results file\n",
    "if read_fap == True:\n",
    "    fname1 = SAVEPATH+name_string+'_'+strfcount(fcount-1)+'_bootstrap_max_npgram_trials_'+str(Np)+'.txt'\n",
    "    max_npgram_trial = np.loadtxt(fname1, usecols=(0))\n",
    "\n",
    "if LDEBUG >= 1: print('Min/Max peak height in bootstrap simulation:', min(max_npgram_trial), max(max_npgram_trial))\n",
    "\n",
    "# define histogram parameters\n",
    "hdstep = (maxbin-minbin)/nhbins   # histogram bin width\n",
    "hbins = [(minbin + hdstep*xx) for xx in range(nhbins+1)]   # histogram bin centers\n",
    "\n",
    "if LDEBUG >= 1: print('Min/Max histogram bins for peak heights:', hbins[0], hbins[-1])\n",
    "\n",
    "# evaluate the histogram\n",
    "values, base = np.histogram(max_npgram_trial, bins=hbins)\n",
    "# evaluate the cumulative probability function\n",
    "cumulative = np.cumsum(values)\n",
    "# plot the cumulative probability function\n",
    "plt.plot(base[:-1], cumulative/max(cumulative), c='blue', zorder=3)\n",
    "# plot the survival probability function\n",
    "#plt.plot(base[:-1], 1.0-cumulative/max(cumulative), c='red')\n",
    "\n",
    "# find peak heights at various confidence levels\n",
    "fap_i = []\n",
    "fap_x = []\n",
    "for lval in fap_levels:\n",
    "    idx = np.where(cumulative/max(cumulative) >= lval)\n",
    "    fap_i.append(idx[0][0])\n",
    "    fap_x.append(base[fap_i[-1]])\n",
    "\n",
    "if LDEBUG >= 2: print(idx90, idx95, idx99)\n",
    "if LDEBUG >= 1: \n",
    "    print('Np = ', Np)\n",
    "    print('Peak height limits for FAP confidence levels of:')\n",
    "    print([str(f\"{ii*100:.2f}\")+'% = '+str(f\"{jj:.6f}\") for ii, jj in zip(fap_levels, fap_x)])\n",
    "\n",
    "# visualize the cumulative probability distribution\n",
    "plt.axis([0.002, 0.009, -0.04, 1.04])\n",
    "\n",
    "# plot decorations\n",
    "plt.minorticks_on()\n",
    "plt.xticks(fontsize=pfontsize1)\n",
    "plt.yticks(fontsize=pfontsize1)\n",
    "plt.xlabel('Normalized Periodogram Maximum Peak Height',fontsize=pfontsize2, labelpad=10)\n",
    "plt.ylabel('Normalized Cumulative Counts', fontsize=pfontsize2, labelpad=20)\n",
    "# y = 0, 1 lines (zorder controls order in which lines are drawn; lower = earlier)\n",
    "plt.axhline(0, 0, 1, c='black', ls='--', lw=1, zorder=1)\n",
    "\n",
    "# confidence level markers\n",
    "for i in range(len(fap_levels)):\n",
    "    plt.plot([minbin, fap_x[i]], [fap_levels[i], fap_levels[i]], c='black', ls='-', lw=1)\n",
    "    plt.plot([fap_x[i], fap_x[i]], [-1, fap_levels[i]], c='black', ls='-', lw=1)\n",
    "\n",
    "# output the visualization\n",
    "plt.savefig(SAVEPATH+name_string+'_'+strfcount(fcount)+'_fap_cumulative_trials_'+str(Np)+'.png',dpi=150, bbox_inches='tight')\n",
    "\n",
    "if LDEBUG >= 0: \n",
    "    print('\\n')\n",
    "    print('*** Finished determining FAP levels from bootstrap simulation results')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Identify and characterize the peaks in the periodogram\n",
    "Use Gaussian fitting to find centers (frequencies) and hwhm (uncertainties) of the periodogram peaks above a threshhold value. Assign a FAP confidence level based on the height of each peak."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################\n",
    "# CUSTOMIZE ANY OF THESE AS NEEDED!!!\n",
    "\n",
    "# set height threshold for finding peaks \n",
    "phlim = fap_x[-1]\n",
    "\n",
    "# change plot dimensions\n",
    "plt.rcParams[\"figure.figsize\"] = (20,10)\n",
    "\n",
    "# show individual Gaussian fitting peak plots\n",
    "show_plots = False\n",
    "#show_plots = True\n",
    "###############################################\n",
    "\n",
    "fcount = 5 # increment output file counter\n",
    "\n",
    "# import specific packages needed here\n",
    "from scipy.optimize import curve_fit\n",
    "from scipy.signal import find_peaks\n",
    "\n",
    "# define Gaussian fitting function\n",
    "def gaussfunc(gx,ga,gx0,gsigma):\n",
    "    return ga*np.exp(-(gx-gx0)**2/(2*gsigma**2))\n",
    "\n",
    "if LDEBUG >= 2: \n",
    "    # write peak finding/fitting results to output file\n",
    "    fname1 = SAVEPATH+name_string+'_'+strfcount(fcount)+'_periodogram_peaks_trials_'+str(Np)+'.txt'\n",
    "    outfile1 = open(fname1, 'w')\n",
    "\n",
    "    # write peak finding/fitting results to output file in an alternate format\n",
    "    fname2 = SAVEPATH+name_string+'_'+strfcount(fcount)+'_periodogram_peaks_periods_trials_'+str(Np)+'.txt'\n",
    "    outfile2 = open(fname2, 'w')\n",
    "\n",
    "# visualize the unfolded light curve data (diagnostic only - not intended for publication)\n",
    "plt.subplots_adjust(hspace=0.35)\n",
    "plt.subplot(3, 1, 1)\n",
    "plt.plot(x-x[0], y, 'b+')\n",
    "plt.xlabel('Time (d)', size=pfontsize1)\n",
    "plt.ylabel('Amplitude', size=pfontsize1)\n",
    "plt.minorticks_on()\n",
    "\n",
    "# visualize the periodogram (diagnostic only - not intended for publication)\n",
    "plt.subplots_adjust(hspace=0.35)\n",
    "plt.subplot(3, 1, 2)\n",
    "plt.plot(f, npgram)\n",
    "plt.xlabel('Frequency (d$^{-1}$)', size=pfontsize1)\n",
    "plt.ylabel('Normalized Amplitude', size=pfontsize1)\n",
    "plt.axis([fmin, fmax, 0, 1.1*max(npgram)])\n",
    "\n",
    "# find peaks in the periodogram using height threshhold from FAP plot (above)\n",
    "peaks, _ = find_peaks(npgram, height=phlim)\n",
    "\n",
    "if LDEBUG >= 2: \n",
    "    for i in range(len(peaks)):\n",
    "        print(peaks[i], '  ', f[peaks[i]], '  ', 1.0/f[peaks[i]], '  ', npgram[peaks[i]])\n",
    "\n",
    "if LDEBUG >= 2: \n",
    "    message = 'Results of Gaussian fits to periodogram peaks'\n",
    "    print(message)\n",
    "    print(message, file=outfile1)\n",
    "\n",
    "# define DataFrame for holding periodogram peaks info\n",
    "peaks_colnames = ['j', 'kmin', 'kmax', 'npts', 'height', 'center', 'sigma', 'HWHM', 'period', 'unc', 'FAP-confidence']\n",
    "df_pd_peaks = pd.DataFrame(columns=peaks_colnames)\n",
    "\n",
    "# Start the peak-fitting process\n",
    "if show_plots == False:\n",
    "    if LDEBUG >= 2: \n",
    "        message1 = '    j  kmin  kmax  npts  height     center     sigma      HWHM       period     unc         FAP-confidence'\n",
    "        print(message1)\n",
    "        print(message1, file=outfile1)\n",
    "\n",
    "if LDEBUG >= 2:\n",
    "    print(fap_levels)\n",
    "    print(fap_x)\n",
    "\n",
    "# Loop through the identified peaks\n",
    "peak_fit_center = []\n",
    "for i in range(len(peaks)):\n",
    "    j = peaks[i]\n",
    "    \n",
    "    # assign FAP score\n",
    "    for ii in range(len(fap_x)):\n",
    "        if LDEBUG >= 2: print(ii, j, npgram[j], fap_x[ii], fap_levels[ii])\n",
    "        if npgram[j] >= fap_x[ii]:\n",
    "            fap_out = '>'+str(f\"{fap_levels[ii]*100:.2f}\")+'%'\n",
    "            break\n",
    "\n",
    "    # Find range around peak for the Gaussian fit (start at peak and count points \n",
    "    # in both directions until the \"next\" point is higher than the current one)\n",
    "    # find range shortward of peak\n",
    "    kmin = j\n",
    "    while npgram[kmin-1] < npgram[kmin]:\n",
    "        kmin = kmin-1\n",
    "    # find range longward of peak\n",
    "    kmax = j\n",
    "    while npgram[kmax+1] < npgram[kmax]:\n",
    "        kmax = kmax+1\n",
    "    kmax += 1\n",
    "    \n",
    "    # define sub-arrays for Gaussian fitting\n",
    "    gx = f[kmin:kmax]\n",
    "    gy = npgram[kmin:kmax]\n",
    "    gn = len(gx)                          \n",
    "    gmean = sum(gx*gy)/sum(gy)               \n",
    "    gsigma = np.sqrt(sum(gy * (gx - gmean)**2) / sum(gy))\n",
    "    \n",
    "    # perform Gaussian fitting and recover fit parameters\n",
    "    popt,pcov = curve_fit(gaussfunc,gx,gy,p0=[npgram[j],gmean,gsigma])\n",
    "    if LDEBUG >= 2: \n",
    "        print([npgram[j],gmean,gsigma])\n",
    "        print(popt)\n",
    "    rampl = round(popt[0],8)\n",
    "    rcent = round(popt[1],8)\n",
    "    rsigm = round(popt[2],8)\n",
    "    rhwhm = round(rsigm*np.sqrt(2.0*np.log(2.0)),8)\n",
    "    \n",
    "    rperd = round(1.0/popt[1],8)\n",
    "    rpunc = round(((rsigm*np.sqrt(2.0*np.log(2.0)))/popt[1] * (1.0/popt[1])), 8)\n",
    "    \n",
    "    peak_fit_center.append(rcent)\n",
    "    \n",
    "    # optionally show zoomed plots of individual peaks with overlaid Gaussian fitting results\n",
    "    if show_plots == True:\n",
    "        plt.subplot(3, 1, 3)\n",
    "        plt.plot(f, npgram)\n",
    "        plt.xlabel('Frequency (d$^{-1}$)', size=pfontsize1)\n",
    "        plt.ylabel('Normalized Amplitude', size=pfontsize1)\n",
    "        plt.axis([f[kmin-10], f[kmax+10], 0, 1.1*max(npgram[kmin:kmax])])\n",
    "        if LDEBUG >= 2: \n",
    "            print(f[kmin-10], f[kmax+10], 0, 1.1*max(npgram[kmin:kmax]))\n",
    "            for ii in peaks:\n",
    "                plt.plot(f[ii], npgram[ii], \"rx\")\n",
    "        plt.plot(gx,gaussfunc(gx,*popt),'ro:',label='fit')\n",
    "        plt.pause(0.01)\n",
    "        if LDEBUG >= 1: print(message1)\n",
    "    \n",
    "    if LDEBUG >= 2: \n",
    "        message = \"{:5d} {:5d} {:5d} {:5d}  {:.8f} {:.8f} {:.8f} {:.8f} {:.8f} {:.8f} {}\".format(j, kmin, kmax, kmax-kmin, rampl, rcent, rsigm, rhwhm, rperd, rpunc, fap_out)\n",
    "        print(message)\n",
    "    \n",
    "    # append fit parameters to DataFrame\n",
    "    df_params = [int(j), int(kmin), int(kmax), int(kmax-kmin), rampl, rcent, rsigm, rhwhm, rperd, rpunc, fap_out]\n",
    "    df_pd_peaks = df_pd_peaks.append(pd.Series(df_params, index=df_pd_peaks.columns), ignore_index=True)\n",
    "    \n",
    "    if LDEBUG >= 2:\n",
    "        pcnt = str(i)\n",
    "        if i < 10: pcnt = '0'+pcnt\n",
    "        pcnt = pcnt+' ='\n",
    "        if rperd < 100: pcnt = pcnt+' '\n",
    "        if rperd < 10: pcnt = pcnt+' '\n",
    "        message2 = \"{} {:.8f} {} {:.8f} {} {:.8f} {} {:.8f} {} {:.8f} {}\".format('period'+pcnt, rperd, ' # unc =', rpunc, ' h =', rampl, ' freq =', rcent, ' HWHM =', rhwhm, fap_out)\n",
    "        print(message, file=outfile1)\n",
    "        print(message2, file=outfile2)\n",
    "\n",
    "# restore default plot dimensions\n",
    "plt.rcParams[\"figure.figsize\"] = (12,6)\n",
    "\n",
    "# restore default plot size\n",
    "plt.rcParams[\"figure.figsize\"] = PSIZE_DEFAULT\n",
    "\n",
    "if LDEBUG >= 2:\n",
    "    # close output files\n",
    "    outfile1.close()\n",
    "    outfile2.close()\n",
    "\n",
    "if LDEBUG >= 0:\n",
    "    print(df_pd_peaks)\n",
    "    print('*** Finished identification and gaussian fits of periodogram peaks')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Visualize periodogram with expanded scale and identified peaks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################\n",
    "# CUSTOMIZE ANY OF THESE AS NEEDED!!!\n",
    "\n",
    "# number of panels to show\n",
    "npanels = 3\n",
    "\n",
    "# update this dictionary to include settings for each panel\n",
    "#    xmin, xmax = minimum, maximum frequency to plot\n",
    "#    ymin, ymax = minimum, maximum power to plot\n",
    "#    y_minor_tick = size of y-axis minor tick intervals\n",
    "if tidx == 0:\n",
    "    panparms = ( {\n",
    "    'xmin':[0.0,  5.1, 10.4],\n",
    "    'xmax':[0.8,  5.9, 11.2],\n",
    "    'ymin':[0.0,  0.0,  0.0],\n",
    "    'ymax':[0.20, 0.20, 0.20],\n",
    "    'y_minor_tick':[0.010, 0.010, 0.010]\n",
    "    } )\n",
    "if tidx == 1:\n",
    "    panparms = ( {\n",
    "    'xmin':[0.0,  5.1, 10.4],\n",
    "    'xmax':[0.8,  5.9, 11.2],\n",
    "    'ymin':[0.0,  0.0,  0.0],\n",
    "    'ymax':[0.005, 0.005, 0.005],\n",
    "    'y_minor_tick':[0.001, 0.001, 0.001]\n",
    "    } )\n",
    "###############################################\n",
    "\n",
    "fcount = 6 # increment output file counter\n",
    "\n",
    "# handle obvious error(s)\n",
    "if tidx > len(tname)-1:\n",
    "    if LDEBUG >= 0: print('ERROR: No plot window settings defined')\n",
    "\n",
    "# set up the multipanel plot\n",
    "fig, axes = plt.subplots(npanels, 1, figsize=(10,npanels*3))\n",
    "\n",
    "# centered common axis labels\n",
    "fig.text(0.5, 0.04, 'Frequency (d$^{-1}$)', ha='center', size=pfontsize2)\n",
    "fig.text(0.02, 0.5, 'Normalized Amplitude', va='center', rotation='vertical', size=pfontsize2)\n",
    "\n",
    "# set font size for axis tick labels\n",
    "plt.rc('xtick', labelsize=pfontsize1) \n",
    "plt.rc('ytick', labelsize=pfontsize1) \n",
    "\n",
    "# loop over panels\n",
    "for i, ax in enumerate(axes.flatten()):\n",
    "    # plot data into current panel\n",
    "    if LDEBUG >= 2: print(i, ax)\n",
    "    ax.plot(f, npgram)\n",
    "\n",
    "    # plot decorations\n",
    "    pstep = panparms['y_minor_tick'][i]\n",
    "    ax.axis([panparms['xmin'][i], panparms['xmax'][i], panparms['ymin'][i]-2*panparms['y_minor_tick'][i], panparms['ymax'][i]+2*panparms['y_minor_tick'][i]], fontsize=pfontsize1)\n",
    "    ax.minorticks_on()\n",
    "    ax.yaxis.set_minor_locator(MultipleLocator(panparms['y_minor_tick'][i]))\n",
    "    \n",
    "    # plot markers for the gaussian fit peak centers from the previous cell\n",
    "    for ii in range(len(peak_fit_center)):\n",
    "        j = peaks[ii]\n",
    "        if LDEBUG >= 2: print(j, peaks[ii], npgram[j])\n",
    "        if npgram[j] >= fap_x[2]:\n",
    "            ax.plot([peak_fit_center[ii],peak_fit_center[ii]], [npgram[j]-pstep,npgram[j]+pstep], color='red')\n",
    "\n",
    "# output the visualization\n",
    "plt.savefig(SAVEPATH+name_string+'_'+strfcount(fcount)+'_LSperiodogram_panels.png',dpi=150, bbox_inches='tight')\n",
    "\n",
    "if LDEBUG >= 0: \n",
    "    print('*** Finished plotting periodogram peaks')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Calculate the phases for all periods\n",
    "Converts data time values into phases corresponding to all of the periodogram peaks identified above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "###############################################\n",
    "# CUSTOMIZE ANY OF THESE AS NEEDED!!!\n",
    "\n",
    "# OPTIONAL phase offset to add to calculated phases\n",
    "phi_offset = 0.0\n",
    "\n",
    "# OPTIONAL time zeropoint for calculating phases (-1 to use first time)\n",
    "phi_zeropoint = -1\n",
    "###############################################\n",
    "\n",
    "if phi_zeropoint < 0: phi_zeropoint = df_lc_clean.time.to_numpy()[0]\n",
    "\n",
    "if LDEBUG >= 2: \n",
    "    print('phi_zeropoint =', phi_zeropoint)\n",
    "    print(df_lc_clean.time.to_numpy())\n",
    "\n",
    "# loop over periods\n",
    "for i in range(len(df_pd_peaks.period)):\n",
    "    colname = str(i)\n",
    "    #if i < 100: colname = '0'+colname\n",
    "    if i < 10: colname = '0'+colname\n",
    "    colname = 'phi'+colname\n",
    "    \n",
    "    if LDEBUG >= 2: \n",
    "        print(colname)\n",
    "        print(df_pd_peaks.period.to_numpy()[i])\n",
    "    \n",
    "    # add new column for each set of phases to existing DataFrame\n",
    "    df_lc_clean[colname] = (df_lc_clean.time.to_numpy() - phi_zeropoint)/df_pd_peaks.period.to_numpy()[i] + phi_offset\n",
    "        \n",
    "if LDEBUG >= 2: print(df_lc_clean)\n",
    "\n",
    "if LDEBUG >= 0:\n",
    "    print('*** Finished calculating phases')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Phase binning\n",
    "Average the phases for a user-selected period into a user-defined number of bins per cycle. The result is a binned light curve for each cycle of the period, and a final \"average\" light curve for the entire phase-folded data set for the period."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "###############################################\n",
    "# CUSTOMIZE ANY OF THESE AS NEEDED!!!\n",
    "\n",
    "# step size for phase bins (1/dstep should be an integer)\n",
    "dstep = 0.10\n",
    "\n",
    "# select period from list of periods defined above (df_pd_peaks.periods)\n",
    "pidx = 8\n",
    "###############################################\n",
    "\n",
    "# function to truncate a number to keep only the decimal part (used with phase calculations)\n",
    "def mphi(p):\n",
    "    return p-int(p)\n",
    "\n",
    "# half of a phase bin width\n",
    "dshift = dstep/2.0\n",
    "# number of phase bins\n",
    "nphibins = int(1.0/dstep)\n",
    "\n",
    "if LDEBUG >= 1: print('nphibins = ',nphibins)\n",
    "\n",
    "# calculate phase bin starting points\n",
    "pbins = [(0.0 + dstep*x)-(dstep/2.0) for x in range(nphibins+1)]\n",
    "pbins[0] = pbins[0]+1.0\n",
    "pbins = [round(i,3) for i in pbins]\n",
    "\n",
    "if LDEBUG >= 2: print(pbins)\n",
    "\n",
    "# calculate centers of phase bins\n",
    "pcbins = [(0.0 + dstep*x) for x in range(nphibins)]\n",
    "pcbins.append(pcbins[0]+1.0)\n",
    "pcbins = [round(i,3) for i in pcbins]\n",
    "\n",
    "if LDEBUG >= 1: print('Bin centers:', pcbins)\n",
    "\n",
    "# sort data into phase bins\n",
    "spidx = str(pidx)\n",
    "if pidx < 10: spidx = '0'+spidx\n",
    "p = np.copy(df_lc_clean['phi'+spidx])\n",
    "a = np.copy(df_lc_clean['varflux'])\n",
    "\n",
    "# define new DataFrame to store phase-binned data\n",
    "phi_colnames1 = [str(i) for i in pcbins]\n",
    "phi_colnames2 = [str(i+1) for i in pcbins]\n",
    "phi_colnames = phi_colnames1 + phi_colnames2[1:]\n",
    "df_lc_clean_bin = pd.DataFrame(columns=phi_colnames)\n",
    "\n",
    "if LDEBUG >= 2: print(df_lc_clean_bin)\n",
    "\n",
    "# running sum for each phase bin for all cycles\n",
    "allbinsum = [0.0 for i in range(nphibins)]\n",
    "# running total of data points summed into each phase bin for all cycles\n",
    "allbinnum = [0 for i in range(nphibins)]\n",
    "# average in each phase bin for all cycles (default is Nan)\n",
    "allbinavg = [np.nan for i in range(nphibins)]\n",
    "    \n",
    "# loop over all phase cycles in input data\n",
    "for i in range(int(np.floor(max(p)))):\n",
    "    # running sum for each phase bin for one cycle\n",
    "    binsum = [0.0 for i in range(nphibins)]\n",
    "    # running total of data points summed into each phase bin for one cycle\n",
    "    binnum = [0 for i in range(nphibins)]\n",
    "    # average in each phase bin for one cycle (default is Nan)\n",
    "    binavg = [np.nan for i in range(nphibins)]\n",
    "    \n",
    "    subp = p[(p >= i) & (p < i+1)]\n",
    "    suba = a[(p >= i) & (p < i+1)]    \n",
    "    \n",
    "    if len(subp) > 0:\n",
    "        if LDEBUG >= 2: \n",
    "            print(subp)\n",
    "            print(suba)\n",
    "        \n",
    "        # truncate phases to keep just decimal part\n",
    "        subpdec = [mphi(ii) for ii in subp]\n",
    "        \n",
    "        # loop over all phase bins\n",
    "        for j in range(nphibins):\n",
    "            # sort appropriate data into current phase bin\n",
    "            for k in range(len(subp)):\n",
    "                if (mphi(subpdec[k]+dshift) >= pcbins[j] and mphi(subpdec[k]+dshift) < pcbins[j+1]):\n",
    "                    allbinsum[j] += suba[k]\n",
    "                    allbinnum[j] += 1\n",
    "                    binsum[j] += suba[k]\n",
    "                    binnum[j] += 1\n",
    "                    if LDEBUG >= 2:\n",
    "                        print('i, j, k:', i, j, k)\n",
    "                        print('pcbins[j], pcbins[j+1], mphi(subpdec[k]+dshift):', pcbins[j], pcbins[j+1], mphi(subpdec[k]+dshift))\n",
    "                        print('binsum, binnum:', binsum[j], binnum[j])\n",
    "                        print('allbinsum, allbinnum:', allbinsum[j], allbinnum[j])\n",
    "            # calculate average value in current bin\n",
    "            if binnum[j] > 0: binavg[j] = binsum[j]/binnum[j]\n",
    "        \n",
    "        # append copy of phase bin averages to provide two cycles of phased data\n",
    "        for ii in range(nphibins):\n",
    "            binavg.append(binavg[ii])\n",
    "        binavg.append(binavg[0])\n",
    "\n",
    "        if LDEBUG >= 2: print(binsum, binnum, binavg)\n",
    "        \n",
    "        # store average phase bin values for this cycle in a row of the dataframe\n",
    "        df_lc_clean_bin = df_lc_clean_bin.append(pd.Series(binavg, index=df_lc_clean_bin.columns), ignore_index=True)\n",
    "    else:\n",
    "        if LDEBUG >= 0: print('***ERROR: no data for phase cycle', i)\n",
    "\n",
    "    if LDEBUG >= 2:\n",
    "        print('\\n')\n",
    "        print('===========================================================')\n",
    "        print('\\n')\n",
    "\n",
    "# calculate average value in phase bins for all cycles\n",
    "for j in range(nphibins):\n",
    "    if allbinnum[j] > 0: allbinavg[j] = allbinsum[j]/allbinnum[j]\n",
    "        \n",
    "# append copy of phase bin averages to provide two cycles of phased data\n",
    "for ii in range(nphibins):\n",
    "    allbinavg.append(allbinavg[ii])\n",
    "allbinavg.append(allbinavg[0])        \n",
    "\n",
    "# store average phase bin values for all cycles in last row of the DataFrame\n",
    "# This is the \"average\" light curve for each period\n",
    "df_lc_clean_bin = df_lc_clean_bin.append(pd.Series(allbinavg, index=df_lc_clean_bin.columns), ignore_index=True)\n",
    "\n",
    "if LDEBUG >= 2:\n",
    "    print('\\n')\n",
    "    print(df_lc_clean_bin)\n",
    "\n",
    "if LDEBUG >= 0: \n",
    "    print('\\n')\n",
    "    print('*** Finished phase binning')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 2D LIGHT CURVE\n",
    "Visualize the data as a 2-dimensional light curve (X = phase, Y = cycles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "###############################################\n",
    "# CUSTOMIZE ANY OF THESE AS NEEDED!!!\n",
    "\n",
    "# Select size(s) of the moving window for smoothing (number of cycles, [min, max])\n",
    "# Set min to be the smallest number that eliminates all Nans (white pixels) in the visualization\n",
    "wroll = [ 5, 20]\n",
    "\n",
    "# color map name\n",
    "pcmap = 'viridis'\n",
    "#pcmap = 'plasma'\n",
    "#pcmap = 'magma'\n",
    "#pcmap = 'inferno'\n",
    "#pcmap = 'cividis'\n",
    "\n",
    "# aspect ratio (1.0 = 1:1)\n",
    "pasp = 0.015\n",
    "###############################################\n",
    "\n",
    "fcount = 7 # increment output file counter\n",
    "\n",
    "# define the number of rows in the visualization image\n",
    "numrows = len(df_lc_clean_bin.index)\n",
    "\n",
    "# set up the multipanel visualization\n",
    "gs_kw=dict(wspace=0.02)\n",
    "fig, axN = plt.subplots(1,3, sharey=True, figsize=(9.6,8), constrained_layout=True, gridspec_kw=gs_kw)\n",
    "\n",
    "# offsets for axis labels\n",
    "plxpos = [ 0.45, -0.04]\n",
    "plypos = [-0.05 ,  0.50]\n",
    "\n",
    "# common axis labels shared by subplot panels\n",
    "fig.text(plxpos[0], plxpos[1], 'Phase (period = '+str(df_pd_peaks.period[pidx])+' d)', ha='center', size=pfontsize2)\n",
    "fig.text(plypos[0], plypos[1], 'Cycles', va='center', rotation='vertical', size=pfontsize2)\n",
    "\n",
    "# Visualize the data as a 2-dimensional light curve\n",
    "# Three panels show: (1) raw data, (2 & 3) data smoothed by a moving window average (window widths defined above)\n",
    "imethod = 'none'\n",
    "for i in range(0,3):\n",
    "    # make new DataFrame for working in\n",
    "    df_lc_clean_binavg = df_lc_clean_bin.copy()\n",
    "    # convert amplitudes to percentage variability\n",
    "    for colname in phi_colnames:\n",
    "        df_lc_clean_binavg[colname] = 100.0*(df_lc_clean_bin[colname] - varflux_mean) / varflux_mean\n",
    "    \n",
    "    # apply moving window smoothing\n",
    "    if i >= 1:\n",
    "        for colname in phi_colnames:\n",
    "            df_lc_clean_binavg[colname] = df_lc_clean_binavg[colname].rolling(window=wroll[i-1], min_periods=1, center=True).mean()\n",
    "\n",
    "    # label subplots\n",
    "    if i == 0:\n",
    "        ltitle = 'Raw'   # Missing data (Nans) are visualized in white\n",
    "    else:\n",
    "        ltitle = 'Navg='+str(wroll[i-1])\n",
    "\n",
    "    # visualize data into subplots\n",
    "    im = axN[i].imshow(df_lc_clean_binavg,aspect=pasp,origin='lower',extent=[0.0,2.0,0,numrows],interpolation=imethod,cmap=pcmap)\n",
    "    \n",
    "    # format and label tickmarks\n",
    "    axN[i].tick_params(labelsize=pfontsize1, length=5)\n",
    "    axN[i].set_xticks((0, 0.5, 1, 1.5, 2))\n",
    "    axN[i].set_xticklabels(('0','0.5','1','1.5','2'))\n",
    "    axN[i].set_title('('+abclabels[i]+') '+ltitle, fontsize=pfontsize1, pad=10)\n",
    "\n",
    "    # display color bar\n",
    "    if i == 2:\n",
    "        cbar = fig.colorbar(im, ax=axN[i], shrink=0.8)\n",
    "        cbar.ax.tick_params(labelsize=pfontsize1)\n",
    "        cbar.set_label('Flux Variation (%)', fontsize=pfontsize2, labelpad=28, rotation=270)\n",
    "\n",
    "# output the visualization\n",
    "fig.savefig(SAVEPATH+name_string+'_'+strfcount(fcount)+'_lc_trailed_'+str(df_pd_peaks.period[pidx])+'.png',dpi=150, bbox_inches='tight')\n",
    "\n",
    "if LDEBUG >= 0: \n",
    "    print('*** Finished displaying 2-dimensional trailed light curves')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# REFERENCES\n",
    "\n",
    "## General\n",
    "\n",
    "1. Command continuation on multiple lines:\n",
    "    - https://stackoverflow.com/questions/17076710/python-split-statement-into-multiple-lines\n",
    "1. Comparison of interpolation methods:\n",
    "    - https://matplotlib.org/3.1.0/gallery/images_contours_and_fields/interpolation_methods.html#sphx-glr-gallery-images-contours-and-fields-interpolation-methods-py\n",
    "1. Gaussian fit:\n",
    "    - https://stackoverflow.com/questions/19206332/gaussian-fit-for-python\n",
    "1. Multiprocessing:\n",
    "    - https://www.praetorian.com/blog/multi-core-and-distributed-programming-in-python?edition=2019\n",
    "1. Robust random numbers (use SystemRandom):\n",
    "    - https://smallbusiness.chron.com/randomize-list-python-26724.html\n",
    "\n",
    "\n",
    "## Periodogram\n",
    "\n",
    "1. Cumulative distribution plot:\n",
    "    - https://stackoverflow.com/questions/15408371/cumulative-distribution-plots-python\n",
    "1. Confidence level examples:\n",
    "    - https://www-zeuthen.desy.de/students/2017/Summerstudents2017/reports/GiulianaNoto.pdf (Section 3.4)\n",
    "1. False Alarm Probability (FAP)\n",
    "    - https://arxiv.org/pdf/1703.09824.pdf (Bootstrapping the FAP; Section 7.4.2)\n",
    "    - https://mail.python.org/pipermail//astropy/2016-June/003937.html\n",
    "1. Non-Uniform Nyquist Limit:\n",
    "    - https://arxiv.org/pdf/1703.09824.pdf (Section 4.1.2)\n",
    "1. Normalization:\n",
    "    - https://github.com/scipy/scipy/issues/2162\n",
    "1. Number of periodogram frequencies:\n",
    "    - https://arxiv.org/pdf/1703.09824.pdf (Section 7.1)\n",
    "\n",
    "\n",
    "## Plotting in Python\n",
    "\n",
    "1. Annotation (matplotlib.pyplot.annotate):\n",
    "    - https://matplotlib.org/3.1.1/api/_as_gen/matplotlib.pyplot.annotate.html\n",
    "1. Axis labels:\n",
    "    - https://stackoverflow.com/questions/33283601/manually-defined-axis-labels-for-matplotlib-imshow/33283892\n",
    "1. Colorbars:\n",
    "    - https://stackoverflow.com/questions/18266642/multiple-imshow-subplots-each-with-colorbar\n",
    "    - https://stackoverflow.com/questions/40184696/change-fontsize-of-colorbars-in-matplotlib\n",
    "1. Colormaps:\n",
    "    - https://matplotlib.org/3.1.0/tutorials/colors/colormaps.html\n",
    "1. Multipanel plot using a FOR loop\n",
    "    - https://stackoverflow.com/questions/42845887/how-to-run-a-smart-loop-for-subplots-in-python\n",
    "1. Spacing of subplots:\n",
    "    - https://stackoverflow.com/questions/37864735/matplotlib-and-ipython-notebook-displaying-exactly-the-figure-that-will-be-save\n",
    "    - https://stackoverflow.com/questions/6541123/improve-subplot-size-spacing-with-many-subplots-in-matplotlib    \n",
    "1. Tick font size in subplots:\n",
    "    - https://stackoverflow.com/questions/38369188/set-size-of-ticks-in-all-subplots\n",
    "1. Tick parameters:\n",
    "    - https://matplotlib.org/3.1.0/api/_as_gen/matplotlib.axes.Axes.tick_params.html\n",
    "1. Titles:\n",
    "    - https://stackoverflow.com/questions/8248467/matplotlib-tight-layout-doesnt-take-into-account-figure-suptitle\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DEPRECATED CELLS BELOW HERE\n",
    "\n",
    "### I'm still working on these ones..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### L-S Periodogram results for primary target (SAP data)\n",
    "\n",
    "height     |frequency   |HWHM       |period      |uncertainty |FAP-confidence |Identification\n",
    "-----------|------------|-----------|------------|------------|---------------|---------------\n",
    "0.01042072 | 0.02686826 |0.00452284 |37.21864217 |6.26516708  |>99.73%        |\n",
    "0.00746887 | 0.14787698 |0.00595141 |6.76237763  |0.27215638  |>95.45%        | (s-o) = 0.14831066 (beat)\n",
    "0.12133518 | 0.29876237 |0.00613109 |3.34714179  |0.06868882  |>99.73%        |2(s-o) = 0.29662132\n",
    "0.00620124 | 0.33290648 |0.00564914 |3.00384663  |0.05097275  |>90.00%        |\n",
    "0.00805273 | 0.59768136 |0.00667656 |1.67313231  |0.01869018  |>95.45%        |4(s-o) = 0.59324265\n",
    "0.02046610 | 5.17162533 |0.00604744 |0.19336281  |0.00022611  |>99.73%        |2o - s = 5.1738410\n",
    "0.00604823 | 5.30327179 |0.00318854 |0.18856284  |0.00011337  |>68.27%        |\n",
    "0.15916744 | 5.32215166 |0.00551646 |0.18789393  |0.00019475  |>99.73%        |o (orbit)\n",
    "0.00575038 | 5.34030254 |0.00299313 |0.18725531  |0.00010495  |>68.27%        |\n",
    "0.00583585 | 5.35347730 |0.00366114 |0.18679448  |0.00012775  |>68.27%        |\n",
    "0.00824249 | 5.44012019 |0.00417690 |0.18381947  |0.00014114  |>99.73%        |\n",
    "0.19915756 | 5.47046225 |0.00568197 |0.18279991  |0.00018987  |>99.73%        |s (spin)\n",
    "0.00965358 | 5.48971573 |0.00353928 |0.18215879  |0.00011744  |>99.73%        |\n",
    "0.01122384 | 5.76898908 |0.00575909 |0.17334060  |0.00017304  |>99.73%        |3s-2o  = 5.7670832\n",
    "0.03152549 |10.64383961 |0.00557638 |0.09395106  |0.00004922  |>99.73%        |2o     = 10.644303\n",
    "0.12175030 |10.79239150 |0.00562013 |0.09265787  |0.00004825  |>99.73%        | (s+o) = 10.792614\n",
    "0.04996560 |10.94189330 |0.00568209 |0.09139186  |0.00004746  |>99.73%        |2s     = 10.940925\n",
    "0.01483830 |16.26441601 |0.00569277 |0.06148392  |0.00002152  |>99.73%        |2s + o = 16.263077\n",
    "0.01267788 |21.73386808 |0.00582333 |0.04601114  |0.00001233  |>99.73%        |3s + o = 21.733538\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Plot folded light curves\n",
    "\n",
    "fcount = 6 # increment output file counter\n",
    "\n",
    "###############################################\n",
    "# CUSTOMIZE ANY OF THESE AS NEEDED!!!\n",
    "\n",
    "# plot axis ranges [xmin, xmax, ymin, ymax]\n",
    "if tidx == 0:\n",
    "    paxvals = [0,1,-70,150]\n",
    "if tidx == 1:\n",
    "    paxvals = [0,1,-15,15]\n",
    "###############################################\n",
    "\n",
    "npanels = len(lc['period'])\n",
    "\n",
    "# set up the multipanel plot\n",
    "if npanels <= 4:\n",
    "    fig, axes = plt.subplots(npanels, 1, sharex=False, figsize=(10,npanels*5))\n",
    "    plxpos = [0.50, 0.09]\n",
    "    plypos = [0.02, 0.50]\n",
    "else:\n",
    "    fig, axes = plt.subplots(int(np.ceil(npanels/2)), 2, sharex=False, figsize=(20,npanels*2.5))\n",
    "    plxpos = [0.50, 0.08]\n",
    "    plypos = [0.08, 0.50]\n",
    "\n",
    "# increase buffer space between subplot panels\n",
    "plt.subplots_adjust(hspace=0.3)\n",
    "\n",
    "# centered common axis labels\n",
    "fig.text(plxpos[0], plxpos[1], 'Phase', ha='center', size=pfontsize2)\n",
    "fig.text(plypos[0], plypos[1], 'Flux Variation (%)', va='center', rotation='vertical', size=pfontsize2)\n",
    "\n",
    "# set font size for axis tick labels\n",
    "plt.rc('xtick', labelsize=pfontsize1) \n",
    "plt.rc('ytick', labelsize=pfontsize1) \n",
    "\n",
    "if LDEBUG >= 2: print(x[0:10])\n",
    "\n",
    "# loop over panels\n",
    "for i, ax in enumerate(axes.flatten()):\n",
    "    # bail out if there is an odd number of panels in multicolumn format\n",
    "    if i+1 > npanels:\n",
    "        ax.set_visible(False)\n",
    "        break\n",
    "\n",
    "    # plot data into current panel\n",
    "    if LDEBUG >= 2: print(i, ax)\n",
    "    if LDEBUG >= 2: print(lc['phi'][i][0:10])\n",
    "    ax.errorbar(lc['phi'][i], y0, barsabove=True, color='k', marker='o', markersize='2', linewidth=0, elinewidth=0)\n",
    "\n",
    "    # plot decorations\n",
    "    ax.axis(paxvals)\n",
    "    ax.minorticks_on()\n",
    "    ax.yaxis.set_minor_locator(MultipleLocator(10))\n",
    "    ax.set_title('Period = '+lc['speriod'][i], fontsize=pfontsize1, pad=10)\n",
    "\n",
    "plt.savefig(SAVEPATH+name_string+'_'+strfcount(fcount)+'_lc_folded.png',dpi=150, bbox_inches='tight')\n",
    "\n",
    "if LDEBUG >= 0: \n",
    "    print('\\n')\n",
    "    print('*** Finished plotting folded light curves')\n",
    "\n",
    "# REFERENCES\n",
    "# error bars:\n",
    "# https://matplotlib.org/3.1.0/api/_as_gen/matplotlib.pyplot.errorbar.html\n",
    "\n",
    "# make a subplot invisible\n",
    "# https://stackoverflow.com/questions/14694501/delete-a-subplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot folded light curves with time sequence split data\n",
    "\n",
    "fcount = 7 # increment output file counter\n",
    "\n",
    "###############################################\n",
    "# CUSTOMIZE ANY OF THESE AS NEEDED!!!\n",
    "\n",
    "# which period from pdlist to yuse?\n",
    "pidx = 0\n",
    "\n",
    "# plot axis ranges [xmin, xmax, ymin, ymax]\n",
    "if tidx == 0:\n",
    "    paxvals = [0,1,-70,150]\n",
    "if tidx == 1:\n",
    "    paxvals = [0,1,-15,15]\n",
    "\n",
    "# number of plot panels (equivalenet to number of time intervals to split data into)\n",
    "npanels = 6\n",
    "###############################################\n",
    "\n",
    "if LDEBUG >= 1: print('Period = '+lc['speriod'][pidx])\n",
    "\n",
    "# import specific packages\n",
    "import string\n",
    "import pylab\n",
    "\n",
    "# set up the multipanel plot\n",
    "fig, axes = plt.subplots(int(npanels/2), 2, sharex=False, figsize=(10,npanels*1.5))\n",
    "\n",
    "# centered common axis labels\n",
    "fig.text(0.5, 0.04, 'Phase', ha='center', size=pfontsize2)\n",
    "fig.text(0.04, 0.5, 'Flux Variation (%)', va='center', rotation='vertical', size=pfontsize2)\n",
    "\n",
    "# set font size for axis tick labels\n",
    "plt.rc('xtick', labelsize=pfontsize1) \n",
    "plt.rc('ytick', labelsize=pfontsize1) \n",
    "\n",
    "# define x (phi) array for splitting\n",
    "xtemp = lc['phi'][pidx]\n",
    "nx = len(xtemp)\n",
    "nxsplit = int(nx/npanels)\n",
    "panel_labels = list(string.ascii_lowercase)\n",
    "if LDEBUG >= 2: print(nx, nxsplit)\n",
    "    \n",
    "# loop over panels\n",
    "for i, ax in enumerate(axes.flatten()):\n",
    "    # plot data into current panel\n",
    "    if LDEBUG >= 2: print(i, ax)\n",
    "    \n",
    "    splitidx = list(range(i*nxsplit, (i+1)*nxsplit))\n",
    "    \n",
    "    ax.errorbar(xtemp[splitidx], y[splitidx], barsabove=True, color='k', marker='o', markersize='2', linewidth=0, elinewidth=0)\n",
    "\n",
    "    # plot decorations\n",
    "    ax.axis(paxvals)\n",
    "    ax.minorticks_on()\n",
    "    ax.yaxis.set_minor_locator(MultipleLocator(10))\n",
    "    ax.set_title('('+panel_labels[i]+')', fontsize=pfontsize1, pad=10)\n",
    "\n",
    "plt.savefig(SAVEPATH+name_string+'_'+strfcount(fcount)+'_lc_folded_time_split.png',dpi=150, bbox_inches='tight')\n",
    "\n",
    "if LDEBUG >= 0: \n",
    "    print('\\n')\n",
    "    print('*** Finished plotting folded light curves')\n",
    "\n",
    "# REFERENCES\n",
    "# error bars:\n",
    "# https://matplotlib.org/3.1.0/api/_as_gen/matplotlib.pyplot.errorbar.html\n",
    "\n",
    "# centered common axis labels\n",
    "# https://stackoverflow.com/questions/16150819/common-xlabel-ylabel-for-matplotlib-subplots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overplot cycle-averaged folded light curves\n",
    "\n",
    "fcount = 8 # increment output file counter\n",
    "\n",
    "###############################################\n",
    "# CUSTOMIZE ANY OF THESE AS NEEDED!!!\n",
    "\n",
    "# number of cycles to average per light curve\n",
    "navg = 4\n",
    "\n",
    "# which entry in lc dictionary to use\n",
    "idx = 0\n",
    "###############################################\n",
    "\n",
    "if LDEBUG >= 2:\n",
    "    print(lc)\n",
    "    print(lc['ncycles'][idx]//navg)\n",
    "    print(lc['phi'][idx][10])\n",
    "\n",
    "philist = []\n",
    "for k in range(lc['ncycles'][idx]//navg):\n",
    "    phisum = [0.0 for x in range(len(pbins)-1)]\n",
    "    phinum = [0 for x in range(len(pbins)-1)]\n",
    "    p_lo_lim = phi_int_min+(k*navg)-dshift\n",
    "    p_hi_lim = phi_int_min+((k+1)*navg)-dshift\n",
    "    for i in range(len(x)):\n",
    "        if LDEBUG >= 2: print('A: ',k, i, lc['phi0'][idx][i], p_lo_lim, p_hi_lim, ' - new data point')\n",
    "        if lc['phi0'][idx][i] >= p_lo_lim and lc['phi0'][idx][i] < p_hi_lim:\n",
    "            for j in range(len(pbins)-1):\n",
    "                temp_sum = 0.0\n",
    "                if LDEBUG >= 2: print('B: ',k,i,j,phi[i],pbins[j],pbins[j+1], ' - look for phase bin match')\n",
    "                tphi0 = lc['phi'][idx][i]+dshift\n",
    "                tphi = tphi0 - int(tphi0)\n",
    "                p_lo = pbins[j]+dshift\n",
    "                p_lo = p_lo - int(p_lo)\n",
    "                if tphi >= p_lo:\n",
    "                    p_hi = pbins[j+1]+dshift\n",
    "                    p_hi = p_hi - int(p_hi)\n",
    "                    if p_hi == 0.0:\n",
    "                        p_hi = 1.00\n",
    "                    if tphi < p_hi:\n",
    "                        if LDEBUG >= 2: print('b: ',k,i,j,lc['phi'][idx][i],lc['phi'][idx][i]+dshift,tphi,p_lo,p_hi, ' - look for phase bin match')\n",
    "                        temp_sum += y[i]\n",
    "                        if LDEBUG >= 2: print('C: ',k,i,j,lc['phi'][idx][i],pbins[j],pbins[j+1],y[i],temp_sum, ' - increment phase bin sum and counter')\n",
    "                        phisum[j] += temp_sum\n",
    "                        phinum[j] += 1\n",
    "                        if LDEBUG >= 2: print('D: ',k,i,j,phisum,phinum)\n",
    "                        break\n",
    "        elif lc['phi0'][idx][i] >= p_hi_lim:\n",
    "            break\n",
    "\n",
    "    if LDEBUG >= 2:\n",
    "        print(k,i,j)\n",
    "        print(phisum)\n",
    "        print(phinum)\n",
    "    \n",
    "    phidiv = [(ii/jj) for ii, jj in zip(phisum, phinum)] \n",
    "    if LDEBUG >= 2: print(ii, jj, phidiv)\n",
    "    phidiv.append(phidiv[0])\n",
    "    philist.append(phidiv)\n",
    "    \n",
    "    # plot cycle-averaged folded light curves\n",
    "    plt.errorbar(pcbins, philist[k], barsabove=True, color='k', marker='o', markersize='0', linewidth=1, elinewidth=0)\n",
    "\n",
    "# calculate folded light curve averaged over all cycles\n",
    "phimean = []\n",
    "phierr = []\n",
    "for i in range(nphibins):\n",
    "    phitemp = []\n",
    "    for j in range(lc['ncycles'][idx]//navg):\n",
    "        if LDEBUG >= 2: print(philist[j][i])\n",
    "        phitemp.append(philist[j][i])\n",
    "    phimean.append(np.mean(phitemp))\n",
    "    phierr.append(np.std(phitemp))\n",
    "phimean.append(phimean[0])\n",
    "phierr.append(phierr[0])\n",
    "\n",
    "# plot folded light curve averaged over all cycles\n",
    "plt.errorbar(pcbins, phimean, yerr=phierr, barsabove=True, color='r', marker='o', markersize='12', linewidth=3, elinewidth=3)\n",
    "\n",
    "# plot decorations\n",
    "plt.axis([0,1,-65,150])\n",
    "plt.minorticks_on()\n",
    "plt.xticks(fontsize=pfontsize1)\n",
    "plt.yticks(fontsize=pfontsize1)\n",
    "plt.xlabel('Phase (period = '+lc['speriod'][idx]+' d)',fontsize=pfontsize2, labelpad=10)\n",
    "plt.ylabel('Flux Variation (%)', fontsize=pfontsize2, labelpad=20)\n",
    "\n",
    "plt.savefig(SAVEPATH+name_string+'_'+strfcount(fcount)+'_lc_lines_'+lc['speriod_fname'][idx]+'.png',dpi=150, bbox_inches='tight')\n",
    "\n",
    "if LDEBUG >= 0:\n",
    "    print('\\n')\n",
    "    print('*** Finished plotting cycle-averaged light curves')\n",
    "\n",
    "# REFERENCES\n",
    "# error bars:\n",
    "# https://matplotlib.org/3.1.0/api/_as_gen/matplotlib.pyplot.errorbar.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
